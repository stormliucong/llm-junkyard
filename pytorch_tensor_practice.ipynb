{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "448684b1",
      "metadata": {
        "id": "448684b1"
      },
      "source": [
        "# PyTorch Tensor Operations — Daily Practice Notebook\n",
        "*Generated on 2025-08-19*\n",
        "\n",
        "This notebook is designed for short, daily reps to build fluency with **PyTorch tensor operations**.\n",
        "Mastering these will make writing your own `nn.Module` feel natural.\n",
        "\n",
        "**How to use**  \n",
        "- Work top-to-bottom. Each mini-exercise has tests that will tell you if you got it right.  \n",
        "- Only edit where you see **`# TODO`**.  \n",
        "- Rerun cells after edits until tests pass.  \n",
        "- Aim for 10–20 minutes per day.\n",
        "\n",
        "> Tip: If you’re new to PyTorch, keep the [tensor semantics](https://pytorch.org/docs/stable/tensors.html) docs open."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32dd5ea",
      "metadata": {
        "id": "c32dd5ea"
      },
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "881d6e74",
      "metadata": {
        "id": "881d6e74",
        "outputId": "91fe2423-75cc-449d-dfe4-31f6eaaa049a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "torch.manual_seed(42)\n",
        "torch.set_printoptions(precision=3, sci_mode=False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c56931cd",
      "metadata": {
        "id": "c56931cd"
      },
      "source": [
        "## 2) Warm‑up — creation, dtype, device, reshape/view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "777ac450",
      "metadata": {
        "id": "777ac450",
        "outputId": "19a7bc15-3e5a-4b4b-a893-29deee4b6ceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Warm-up passed.\n"
          ]
        }
      ],
      "source": [
        "# Create a 1-D tensor with values 0..11 and reshape to (3,4)\n",
        "t1 = torch.arange(12, device=device, dtype=torch.int64)\n",
        "t1 = t1.reshape(3,4)\n",
        "\n",
        "# Move to device if available\n",
        "t1 = t1.to(device)\n",
        "\n",
        "# Tests (do not modify)\n",
        "assert t1.shape == (3,4)\n",
        "assert t1.device == device\n",
        "assert t1.dtype == torch.int64\n",
        "print(\"✅ Warm-up passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11cef165",
      "metadata": {
        "id": "11cef165"
      },
      "source": [
        "## 3) Core creation ops — zeros/ones/full/eye/rand/randn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "33083a25",
      "metadata": {
        "id": "33083a25",
        "outputId": "894ed8b9-f60d-4648-f267-102aeb23c118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Creation ops passed.\n"
          ]
        }
      ],
      "source": [
        "# Create the following tensors (on current device):\n",
        "# a) a (2,3) tensor of ones (float32) -> t_ones\n",
        "# b) a (3,3) identity matrix (float32) -> t_eye\n",
        "# c) a (2,2) tensor filled with 7 (int64) -> t_full\n",
        "# d) a (4,5) standard normal -> t_randn\n",
        "# TODO\n",
        "\n",
        "t_ones = torch.ones(2,3, dtype=torch.float32)\n",
        "t_eye = torch.eye(3, dtype=torch.float32)\n",
        "t_full = torch.full((2,2), 7, dtype=torch.float32)\n",
        "t_randn = torch.randn(4, 5, dtype=torch.float32)\n",
        "\n",
        "# Tests\n",
        "assert t_ones.shape == (2,3) and t_ones.dtype == torch.float32\n",
        "assert t_eye.shape == (3,3) and t_eye.trace() == 3\n",
        "assert t_full.dtype == torch.float32 and int(t_full.mean().cpu()) == 7\n",
        "assert t_randn.shape == (4,5)\n",
        "print(\"✅ Creation ops passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01112dca",
      "metadata": {
        "id": "01112dca"
      },
      "source": [
        "## 4) Indexing & slicing — set border to 1, interior to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0de9a18a",
      "metadata": {
        "id": "0de9a18a",
        "outputId": "d845e2cb-c053-4cf2-be65-b60c42b5d9a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Indexing & slicing passed.\n"
          ]
        }
      ],
      "source": [
        "# Create a (5,5) tensor of zeros, set the border to 1, interior stays 0.\n",
        "# Save as t_border.\n",
        "# TODO\n",
        "t_border = torch.zeros(5, 5, device=device, dtype=torch.float32)\n",
        "t_border[: ,0] = 1\n",
        "t_border[-1, :] = 1\n",
        "t_border[0, :] = 1\n",
        "t_border[:, -1] = 1\n",
        "\n",
        "\n",
        "# Tests\n",
        "expected = torch.tensor([[1,1,1,1,1],\n",
        "                         [1,0,0,0,1],\n",
        "                         [1,0,0,0,1],\n",
        "                         [1,0,0,0,1],\n",
        "                         [1,1,1,1,1]], device=device, dtype=t_border.dtype)\n",
        "assert torch.equal(t_border, expected)\n",
        "print(\"✅ Indexing & slicing passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52238fd9",
      "metadata": {
        "id": "52238fd9"
      },
      "source": [
        "## 5) Boolean masks — count values above mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef5ffd12",
      "metadata": {
        "id": "ef5ffd12",
        "outputId": "a0b94446-4eda-4a09-e7e9-d9dc0e94e77f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Boolean mask passed. Count: 14\n"
          ]
        }
      ],
      "source": [
        "# Given a random (6,4) tensor, compute how many values are strictly above its mean.\n",
        "# Save boolean mask to mask, count to count_above.\n",
        "# TODO\n",
        "x = torch.randn(6, 4, device=device, dtype=torch.float32)\n",
        "mask = x > x.mean()\n",
        "count_above = int(mask.sum())\n",
        "\n",
        "\n",
        "# Tests\n",
        "assert mask.dtype == torch.bool and mask.shape == x.shape\n",
        "assert isinstance(count_above, int)\n",
        "print(\"✅ Boolean mask passed. Count:\", count_above)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe39f72",
      "metadata": {
        "id": "abe39f72"
      },
      "source": [
        "## 6) Reshape/view/permute — make contiguous before view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0b8b526e",
      "metadata": {
        "id": "0b8b526e",
        "outputId": "fbd9173b-2791-497e-c819-6a0bf608ef52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.,  1.,  2.,  3.],\n",
            "         [ 4.,  5.,  6.,  7.],\n",
            "         [ 8.,  9., 10., 11.]],\n",
            "\n",
            "        [[12., 13., 14., 15.],\n",
            "         [16., 17., 18., 19.],\n",
            "         [20., 21., 22., 23.]]])\n",
            "tensor([[[ 0.,  1.,  2.,  3.],\n",
            "         [12., 13., 14., 15.]],\n",
            "\n",
            "        [[ 4.,  5.,  6.,  7.],\n",
            "         [16., 17., 18., 19.]],\n",
            "\n",
            "        [[ 8.,  9., 10., 11.],\n",
            "         [20., 21., 22., 23.]]])\n",
            "tensor([[ 0.,  1.,  2.,  3., 12., 13., 14., 15.],\n",
            "        [ 4.,  5.,  6.,  7., 16., 17., 18., 19.],\n",
            "        [ 8.,  9., 10., 11., 20., 21., 22., 23.]])\n",
            "✅ View/permute passed.\n"
          ]
        }
      ],
      "source": [
        "# Start with shape (2,3,4), then permute to (3,2,4).\n",
        "# Try to view it as (3,8) correctly (may require .contiguous()).\n",
        "# Save the final tensor as y_view.\n",
        "# TODO\n",
        "x = torch.arange(24, device=device, dtype=torch.float32)\n",
        "x = x.reshape(2, 3, 4)\n",
        "x = x.permute(1, 0, 2)\n",
        "y_view = x.contiguous().view(3,8)\n",
        "\n",
        "# Tests\n",
        "assert y_view.shape == (3,8)\n",
        "assert torch.equal(y_view[0], torch.tensor([0,1,2,3,12,13,14,15], device=device))\n",
        "print(\"✅ View/permute passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2337b64e",
      "metadata": {
        "id": "2337b64e"
      },
      "source": [
        "## 7) Broadcasting — pairwise L2 distance (no loops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "00c11694",
      "metadata": {
        "id": "00c11694",
        "outputId": "f3765227-2dc8-4dda-c79a-76ca63a8cdee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Broadcasting distance passed.\n"
          ]
        }
      ],
      "source": [
        "# Given A in R^{N x D} and B in R^{M x D}, compute pairwise L2 distances (N x M) using broadcasting.\n",
        "# Use N=4, M=3, D=5 with a fixed seed for reproducibility.\n",
        "# Save result as dist_nm.\n",
        "# Hints: (A[:,None,:] - B[None,:,:])**2 then reduce over D.\n",
        "# TODO\n",
        "N, M, D = 4, 3, 5\n",
        "A = torch.randn(N, D, device=device, dtype=torch.float32)\n",
        "B = torch.randn(M, D, device=device, dtype=torch.float32)\n",
        "\n",
        "dist_nm = torch.sqrt(((A[:,None,:]-B[None,:,:])**2).sum(-1))\n",
        "\n",
        "# Tests\n",
        "assert dist_nm.shape == (N, M)\n",
        "# symmetry on diag when A==B (quick sanity by reusing A)\n",
        "dist_aa = torch.sqrt(((A[:,None,:]-A[None,:,:])**2).sum(-1))\n",
        "assert torch.allclose(torch.diag(dist_aa), torch.zeros(N, device=device), atol=1e-6)\n",
        "print(\"✅ Broadcasting distance passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0274999",
      "metadata": {
        "id": "d0274999"
      },
      "source": [
        "## 8) Reductions — implement stable logsumexp along dim=-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9dae1b",
      "metadata": {
        "id": "ef9dae1b"
      },
      "outputs": [],
      "source": [
        "# Implement a numerically stable logsumexp for a 2D tensor along the last dim.\n",
        "# Save function as def my_logsumexp(x): return ...\n",
        "# TODO\n",
        "def my_logsumexp(x: torch.Tensor) -> torch.Tensor:\n",
        "    pass\n",
        "\n",
        "# Quick test vs PyTorch\n",
        "z = torch.randn(7, 11, device=device) * 5\n",
        "ok = torch.allclose(my_logsumexp(z), torch.logsumexp(z, dim=-1, keepdim=True), atol=1e-6)\n",
        "assert ok\n",
        "print(\"✅ LogSumExp passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c65319",
      "metadata": {
        "id": "f4c65319"
      },
      "source": [
        "## 9) Autograd — verify grad of sum(x**2) is 2x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f24f10c",
      "metadata": {
        "id": "6f24f10c"
      },
      "outputs": [],
      "source": [
        "# Create x requires_grad, compute y = sum(x**2), backprop,\n",
        "# verify x.grad ≈ 2x.\n",
        "# TODO\n",
        "\n",
        "assert torch.allclose(x.grad, 2*x, atol=1e-6)\n",
        "print(\"✅ Autograd gradient check passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d2fa64f",
      "metadata": {
        "id": "3d2fa64f"
      },
      "source": [
        "## 10) Linear layer by hand — y = x @ W.T + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6564c01",
      "metadata": {
        "id": "c6564c01"
      },
      "outputs": [],
      "source": [
        "# Implement a manual linear layer and check against nn.Linear.\n",
        "# Use batch=4, in_features=6, out_features=3.\n",
        "# TODO\n",
        "torch.manual_seed(123)\n",
        "B, Din, Dout = 4, 6, 3\n",
        "x = torch.randn(B, Din, device=device)\n",
        "\n",
        "W = torch.randn(Dout, Din, device=device, requires_grad=True)\n",
        "b = torch.randn(Dout, device=device, requires_grad=True)\n",
        "\n",
        "y_manual =   # TODO\n",
        "\n",
        "layer = nn.Linear(Din, Dout, bias=True).to(device)\n",
        "with torch.no_grad():\n",
        "    layer.weight.copy_(W)\n",
        "    layer.bias.copy_(b)\n",
        "\n",
        "y_ref = layer(x)\n",
        "assert torch.allclose(y_manual, y_ref, atol=1e-6)\n",
        "print(\"✅ Manual linear layer passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5226dd5",
      "metadata": {
        "id": "d5226dd5"
      },
      "source": [
        "## 11) `no_grad`, `detach`, and `requires_grad_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65b6ea5",
      "metadata": {
        "id": "d65b6ea5"
      },
      "outputs": [],
      "source": [
        "# Demonstrate turning grad off for evaluation and re-enabling later.\n",
        "# TODO\n",
        "p = torch.randn(5, device=device, requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    q = p * 3 + 1    # q has no grad history\n",
        "r = (p.detach() * 2).requires_grad_()  # r is a leaf again with grad\n",
        "loss = (r**2).sum()\n",
        "loss.backward()\n",
        "assert p.grad is None  # never used in backward\n",
        "assert r.grad is not None\n",
        "print(\"✅ no_grad/detach demo passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add8f7db",
      "metadata": {
        "id": "add8f7db"
      },
      "source": [
        "## 12) DataLoader — batching tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f576cde",
      "metadata": {
        "id": "2f576cde"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Build a simple dataset and iterate with DataLoader\n",
        "# TODO: create X (100, 8) and y (100,) and a loader with batch_size=16, shuffle=True\n",
        "X = torch.randn(100, 8)\n",
        "y = torch.randint(0, 2, (100,))\n",
        "ds = TensorDataset(X, y)\n",
        "loader = DataLoader(ds, batch_size=16, shuffle=True)\n",
        "\n",
        "batches = 0\n",
        "for xb, yb in loader:\n",
        "    assert xb.shape == (xb.size(0), 8)\n",
        "    assert yb.dim() == 1\n",
        "    batches += 1\n",
        "\n",
        "assert batches == (100 + 16 - 1)//16\n",
        "print(\"✅ DataLoader basics passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd26aa6e",
      "metadata": {
        "id": "bd26aa6e"
      },
      "source": [
        "## 13) Optional — GPU round‑trip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa3ee6fb",
      "metadata": {
        "id": "fa3ee6fb"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    t = torch.randn(2,3, device=\"cuda\")\n",
        "    cpu_t = t.cpu()\n",
        "    assert cpu_t.device.type == \"cpu\"\n",
        "    print(\"✅ GPU round-trip passed.\")\n",
        "else:\n",
        "    print(\"ℹ️ CUDA not available on this machine — skip.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1394f7c5",
      "metadata": {
        "id": "1394f7c5"
      },
      "source": [
        "## 14) Daily Drills (repeatable)\n",
        "Each day, do 2–3 of the prompts below from memory. Then check with PyTorch docs if stuck.\n",
        "\n",
        "1. Build a 3D tensor of shape (2, 3, 4) with integers 0..23. Swap the first two dims and flatten the last two.\n",
        "2. Using boolean masks, set all values in a tensor greater than its (row‑wise) mean to that row’s mean.\n",
        "3. Compute cosine similarities between two matrices `A (N×D)` and `B (M×D)` using only broadcasting and reductions.\n",
        "4. Implement a stable softmax using your `my_logsumexp` (no `F.softmax`).\n",
        "5. Write a function `one_hot(ids, num_classes)` that returns a one‑hot matrix (no loops).\n",
        "6. Vectorize: given `x (N,)`, compute a rolling window mean of width 3 without loops.\n",
        "7. Using `unfold`, implement a 1D valid convolution (stride 1) with a given kernel (no `conv1d`).\n",
        "8. Show three different ways to clone a tensor such that modifying the clone doesn’t change the original.\n",
        "9. Given logits, compute cross‑entropy loss *manually* (no `F.cross_entropy`).\n",
        "10. Do a tiny gradient descent step on a linear model: update `W, b` with `lr=1e-2` using autograd grads."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19de393",
      "metadata": {
        "id": "d19de393"
      },
      "source": [
        "## 15) Bonus — timing: vectorization vs loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e3d4098",
      "metadata": {
        "id": "9e3d4098"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Compare a loop-based pairwise L2 to the vectorized version above.\n",
        "torch.manual_seed(0)\n",
        "N, M, D = 200, 200, 64\n",
        "A = torch.randn(N, D)\n",
        "B = torch.randn(M, D)\n",
        "\n",
        "# Loop version (slow)\n",
        "t0 = time.time()\n",
        "D_loop = torch.empty(N, M)\n",
        "for i in range(N):\n",
        "    for j in range(M):\n",
        "        D_loop[i, j] = torch.norm(A[i] - B[j])\n",
        "t1 = time.time()\n",
        "\n",
        "# Vectorized\n",
        "diff = A[:, None, :] - B[None, :, :]\n",
        "D_vec = torch.sqrt((diff**2).sum(-1))\n",
        "t2 = time.time()\n",
        "\n",
        "print(f\"Loop: {t1 - t0:.3f}s   Vectorized: {t2 - t1:.3f}s   Speedup: {(t1-t0)/(t2-t1+1e-9):.1f}x\")\n",
        "assert torch.allclose(D_loop, D_vec, atol=1e-6)\n",
        "print(\"✅ Timing demo passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9decb779",
      "metadata": {
        "id": "9decb779"
      },
      "source": [
        "## 16) From tensors to Modules — tiny MLP forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17d2f936",
      "metadata": {
        "id": "17d2f936"
      },
      "outputs": [],
      "source": [
        "# Build a minimal MLP using tensor shapes you understand.\n",
        "# 8 -> 16 -> 4 with ReLU, then logits. Confirm output shape.\n",
        "mlp = nn.Sequential(\n",
        "    nn.Linear(8, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 4),\n",
        ")\n",
        "x = torch.randn(32, 8)\n",
        "out = mlp(x)\n",
        "assert out.shape == (32, 4)\n",
        "print(\"✅ Tiny MLP forward passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca47e09d",
      "metadata": {
        "id": "ca47e09d"
      },
      "source": [
        "## 17) Practice checklist\n",
        "- [ ] I can reshape, permute, and `view` safely (with `.contiguous()` when needed).\n",
        "- [ ] I can write pairwise distance and cosine similarity with *broadcasting*.\n",
        "- [ ] I can implement stable `logsumexp` and softmax.\n",
        "- [ ] I can do simple autograd checks.\n",
        "- [ ] I understand `no_grad`, `detach`, and `requires_grad_`.\n",
        "- [ ] I can use `TensorDataset` and `DataLoader`.\n",
        "- [ ] I can wire up a tiny `nn.Sequential` MLP and reason about shapes."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}