{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stormliucong/llm-junkyard/blob/main/Training_a_LLM_with_Shakespear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSqHGcJE3ctk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/gpt2\"\n",
        "import os\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "%cd $path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r18u1jlEuZS3"
      },
      "outputs": [],
      "source": [
        "%pip install nbstripout\n",
        "nbstripout Training-a-LLM-with-Shakespear.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dud_wjceiY1G"
      },
      "outputs": [],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnmWg2-2upR4"
      },
      "source": [
        "# Training of GPT2 style LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sMFKmlVbjQI0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import os\n",
        "# import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import requests\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "import random\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM4zI7Phuw15"
      },
      "source": [
        "## GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDBAVJm-j3BQ"
      },
      "outputs": [],
      "source": [
        "# GPT2 Model\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, rope=True):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(d_model, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.query_linear(query)\n",
        "        key = self.key_linear(key)\n",
        "        value = self.value_linear(value)\n",
        "        # split\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # optional\n",
        "        # RoPE for positional encoding\n",
        "        if self.rotary_embedding is not None:\n",
        "          query = self.rotary_embedding(query)\n",
        "          key = self.rotary_embedding(key)\n",
        "        value = value.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "        # attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        else:\n",
        "            scores = scores.masked_fill(torch.tril(torch.ones_like(scores)) == 0, -1e9)\n",
        "        # softmax\n",
        "        attention = F.softmax(scores, dim=-1) # b, n, s, s\n",
        "        # output\n",
        "        output = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # linear projection\n",
        "        return self.output_linear(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout=0.1, rope=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, heads, rope)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # pre residue connection\n",
        "        residue = x\n",
        "        # attention\n",
        "        x = self.attention(x, x, x, mask)\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm1(x + residue)\n",
        "        # feed forward\n",
        "        residue = x\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm2(x + residue)\n",
        "        return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout=0.1,rope=False):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq, d_model)\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(d_model, heads, d_ff, dropout, rope) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # if mask is None, casual mask will added automatically.\n",
        "        token_emb = self.embedding(x)\n",
        "        pos_emb = self.position_embedding(torch.arange(x.size(1), device=x.device))\n",
        "        x = token_emb + pos_emb\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x, mask)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoPE - Rotary Position Embedding\n",
        "- add position into q and k.\n",
        "- In order to have dot product of q_i, k_j reflect relative position i-j, rotate original by θ_i,\n",
        "- treate x as complext number x = a+bi, rotate θ gives a (cosθ+sinθ) + b (cosθ-sinθ)i. split x into [x1, x2], after rotate [x1cos-x2sin, x1sin+x2cos]\n",
        "- add frequencies for different dimension. f = base^-2d_i/d_model"
      ],
      "metadata": {
        "id": "VqN2_Csp-tcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq, base=10000):\n",
        "        super(RotaryPositionalEmbedding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq = max_seq\n",
        "        self.base = base\n",
        "        # register buffer\n",
        "        self.register_buffer('frequencies', self._get_frequencies())\n",
        "        self.register_buffer('cos_sin', self._get_cos_sin())\n",
        "\n",
        "    def _get_frequencies(self):\n",
        "        frequencies = 1.0 / (self.base ** (torch.arange(0, self.d_model, 2).float() / self.d_model))\n",
        "        return frequencies\n",
        "\n",
        "    def _get_cos_sin(self, pos_i=None):\n",
        "        if pos_i is None:\n",
        "          pos_i = torch.arange(self.max_seq, device=x.device).unsqueeze(1)\n",
        "        else:\n",
        "          pos_i = pos_i.unsqueeze(1)\n",
        "        freqs = self.frequencies.unsqueeze(0)\n",
        "        args = pos_i * freqs\n",
        "        cos = torch.cos(args)\n",
        "        sin = torch.sin(args)\n",
        "        return cos, sin\n",
        "\n",
        "    def _apply_rope(self, x, cos, sin):\n",
        "        # split x into x1, x2\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        # rotate x1, x2\n",
        "        x1 = x1 * self.cos_sin[0] - x2 * self.cos_sin[1]\n",
        "        x2 = x1 * self.cos_sin[1] + x2 * self.cos_sin[0]\n",
        "        # concatenate\n",
        "        return torch.cat([x1, x2], dim=-1)\n",
        "\n",
        "    def forward(self, x, pos_i):\n",
        "        seq_len = x.size(1)\n",
        "        if pos_i is None:\n",
        "            con, sin = self.cos_sin\n",
        "        else:\n",
        "            con, sin = self._get_cos_sin(pos_i)\n",
        "        return self._apply_rope(x, con, sin)\n",
        ""
      ],
      "metadata": {
        "id": "CdcjhgTl_twG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAg22vZyu0jB"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi9LfGm0mgrz"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.seq = self.tokenizer.encode(data)\n",
        "        self.seq_num = len(self.seq) - max_length - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seq_num\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = self.seq[idx:idx+self.max_length]\n",
        "        target = self.seq[idx+1:idx+self.max_length+1]\n",
        "        return torch.tensor(input, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhwqEfFfu6Y3"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0yPy2WTniTC"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "class Trainer:\n",
        "    def __init__(self, model, device, learning_rate = 1e-3, weight_decay = 0.01, warmup_steps = 1000, max_steps = 10000, gradient_accumulation_steps = 1, grad_clip = 1.0, save_dir = \"./checkpoints\"):\n",
        "        self.model = model\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.scheduler = self._get_scheduler(warmup_steps, max_steps)\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.grad_clip = grad_clip\n",
        "        self.save_dir = save_dir\n",
        "        self.global_step = 0\n",
        "        self.best_loss = float('inf')\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    def _get_scheduler(self, warmup_steps, max_steps):\n",
        "        def lr_lambda(current_step):\n",
        "            if current_step < warmup_steps:\n",
        "                return float(current_step) / float(max(1, warmup_steps))\n",
        "            # Cosine decay after warmup\n",
        "            progress = float(current_step - warmup_steps) / float(max(1, max_steps - warmup_steps))\n",
        "            return max(0.0, 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(progress))))\n",
        "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "\n",
        "    def train(self, train_dataloader, validate_dateloader, epochs):\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            total_loss = 0\n",
        "            pb = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                loss = loss / self.gradient_accumulation_steps if self.gradient_accumulation_steps > 1 else loss\n",
        "                loss.backward()\n",
        "                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
        "                    self.optimizer.step()\n",
        "                    self.scheduler.step()\n",
        "                    self.global_step += 1\n",
        "                # update pb for loss\n",
        "                total_loss += loss.item() * self.gradient_accumulation_steps\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / (batch_idx + 1):4f}\")\n",
        "            # call validate\n",
        "            self.validate(validate_dateloader)\n",
        "        print(f\"Training finished\")\n",
        "        return 1\n",
        "\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        pb = tqdm(dataloader, desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                total_loss += loss.item()\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / (batch_idx + 1):4f}\")\n",
        "\n",
        "        if (total_loss / (batch_idx + 1)) < self.best_loss:\n",
        "            self.best_loss = (total_loss / (batch_idx + 1))\n",
        "            self.save_checkpoint()\n",
        "\n",
        "        return 1\n",
        "\n",
        "    def save_checkpoint(self, filename='best_model.pt'):\n",
        "        checkpoint_path = os.path.join(self.save_dir, filename)\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'global_step': self.global_step,\n",
        "            'best_loss': self.best_loss\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    def load_checkpoint(self, filename='best_model.pt'):\n",
        "        checkpoint_path = os.path.join(self.save_dir, filename)\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.global_step = checkpoint['global_step']\n",
        "        self.best_loss = checkpoint['best_loss']\n",
        "        print(f\"Checkpoint loaded from {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP7MOGUQu9-a"
      },
      "source": [
        "## Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4pMf-ff2nxa"
      },
      "outputs": [],
      "source": [
        "class TextGenerator:\n",
        "    def __init__(self, model, tokenizer, device, max_length=100, eos_token=None):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        if eos_token is None:\n",
        "            self.eos_token = self.tokenizer.eot_token\n",
        "        else:\n",
        "            self.eos_token = eos_token\n",
        "\n",
        "    def generate(self, prompt, num_samples=1, temperature=1.0, top_k=50):\n",
        "        tokens = self.tokenizer.encode(prompt)\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0) # 0, s\n",
        "        for _ in range(num_samples):\n",
        "            for _ in range(self.max_length - len(tokens)):\n",
        "                logits = self.model(tokens)\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                filtered_logits = logits.topk(top_k)[0]\n",
        "                probs = F.softmax(filtered_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                if next_token.item() == self.eos_token:\n",
        "                    break\n",
        "            generated_text = self.tokenizer.decode(tokens[0].tolist())\n",
        "            print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oU6EZVWvDJq"
      },
      "source": [
        "## Training Shakespear GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOD1VLrkuN6p"
      },
      "outputs": [],
      "source": [
        "# define all training parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model parameters\n",
        "batch_size = 2\n",
        "d_model = 128\n",
        "max_seq = 128\n",
        "n_layers = 4\n",
        "heads = 4\n",
        "d_ff = 256\n",
        "dropout_rate = 0.1\n",
        "# training parameters\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 1000\n",
        "max_steps = 10000\n",
        "gradient_accumulation_steps = 5\n",
        "grad_clip = 1.0\n",
        "epochs = 10\n",
        "# get gpt2 tokenizer and vocab_size\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab\n",
        "# train and val data\n",
        "if os.path.exists(\"data/t8.shakespeare.txt\"):\n",
        "    # if the file exists, read from it\n",
        "    with open(\"data/t8.shakespeare.txt\", \"r\") as f:\n",
        "        data = f.read()\n",
        "else:\n",
        "    shakespear_content_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "    # read the content from the URL\n",
        "    data = requests.get(shakespear_content_url).text\n",
        "    # save to content to local file\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "    with open(\"data/t8.shakespeare.txt\", \"w\") as f:\n",
        "        f.write(data)\n",
        "train_data, val_data = data[:int(len(data)*0.8)], data[int(len(data)*0.8):]\n",
        "train_dataloader = DataLoader(TextDataset(train_data[1:10000], tokenizer, max_seq), batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(TextDataset(val_data[1:1000], tokenizer, max_seq), batch_size=batch_size, shuffle=True)\n",
        "# init model and trainer\n",
        "model = GPT2(vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout_rate)\n",
        "trainer = Trainer(model, device, learning_rate, weight_decay, warmup_steps, max_steps, gradient_accumulation_steps, grad_clip, \"checkpoints\")\n",
        "# load model if available\n",
        "if os.path.exists(\"checkpoints/best_model.pt\"):\n",
        "    trainer.load_checkpoint()\n",
        "# start training\n",
        "trainer.train(train_dataloader, val_dataloader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUm-ifzevLy2"
      },
      "source": [
        "## Inference on Trained GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWbDPn2JwHu6"
      },
      "outputs": [],
      "source": [
        "# generate\n",
        "# load bst model\n",
        "model = GPT2(vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout_rate)\n",
        "model.load_state_dict(torch.load(\"checkpoints/best_model.pt\")['model_state_dict'])\n",
        "eos_token = tokenizer.eot_token\n",
        "text_generator = TextGenerator(model, tokenizer, device, max_length=100, eos_token=eos_token)\n",
        "prompt = \"he is a \"\n",
        "text_generator.generate(prompt, num_samples=1, temperature=1.0, top_k=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "2eZmcfeg-rK_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "12ezKk_kmj9oUbRsIM6-sAmzGxxSjZ0QR",
      "authorship_tag": "ABX9TyMqZ6b9Z0ThEaUJvKACS4e2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}