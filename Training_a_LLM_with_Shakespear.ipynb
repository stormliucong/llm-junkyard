{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stormliucong/llm-junkyard/blob/main/Training_a_LLM_with_Shakespear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSqHGcJE3ctk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/gpt2\"\n",
        "import os\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "%cd $path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured roadmap to modern LLM/generative AI:\n",
        "\n",
        "### Phase 1: Advanced Architectures (4-6 weeks)\n",
        "\n",
        "**Modern Transformer Variants**\n",
        "- Implement RoPE (Rotary Position Embedding) from scratch - used in LLaMA, PaLM\n",
        "- Build RMSNorm instead of LayerNorm - more stable training\n",
        "- Code SwiGLU activation function - better than ReLU/GELU\n",
        "- Practice: Modify your GPT-2 to use these components\n",
        "\n",
        "**Attention Mechanisms**\n",
        "- Implement Multi-Query Attention (MQA) and Grouped-Query Attention (GQA)\n",
        "- Build FlashAttention-style memory-efficient attention\n",
        "- Add attention variants: Sliding window, sparse attention patterns\n",
        "- Practice: Compare memory usage and speed vs standard attention\n",
        "\n",
        "**Advanced Architectures**\n",
        "- Study and implement key components of LLaMA architecture\n",
        "- Build a simple Mixture of Experts (MoE) layer\n",
        "- Understand and code basic retrieval-augmented generation\n",
        "\n",
        "### Phase 2: Scaling and Efficiency (4-5 weeks)\n",
        "\n",
        "**Distributed Training**\n",
        "- Learn PyTorch DDP (DistributedDataParallel)\n",
        "- Implement gradient accumulation and mixed precision training\n",
        "- Study DeepSpeed ZeRO stages - implement ZeRO-1\n",
        "- Practice: Scale your model training across multiple GPUs\n",
        "\n",
        "**Memory Optimization**\n",
        "- Implement gradient checkpointing\n",
        "- Study and code model sharding techniques\n",
        "- Learn about activation recomputation\n",
        "- Practice: Train larger models with limited memory\n",
        "\n",
        "**Inference Optimization**\n",
        "- Implement KV-cache for faster autoregressive generation\n",
        "- Build basic quantization (INT8, INT4)\n",
        "- Code speculative decoding with a smaller draft model\n",
        "- Study ONNX conversion and TensorRT optimization\n",
        "\n",
        "### Phase 3: Training Methodologies (3-4 weeks)\n",
        "\n",
        "**Advanced Training Techniques**\n",
        "- Implement curriculum learning and data scheduling\n",
        "- Study and code basic RLHF pipeline with reward models\n",
        "- Learn about instruction tuning and chat formatting\n",
        "- Practice: Fine-tune a model for instruction following\n",
        "\n",
        "**Safety and Alignment**\n",
        "- Implement Constitutional AI training loop\n",
        "- Build rejection sampling and best-of-N training\n",
        "- Study red-teaming and safety evaluation methods\n",
        "- Create your own harmlessness dataset and training\n",
        "\n",
        "### Phase 4: Multimodal and Tool Use (4-5 weeks)\n",
        "\n",
        "**Vision-Language Models**\n",
        "- Study CLIP architecture and implement vision encoder\n",
        "- Build simple vision-language connector (linear projection)\n",
        "- Implement basic VQA (Visual Question Answering) model\n",
        "- Practice: Train on image-text pairs\n",
        "\n",
        "**Tool Integration**\n",
        "- Implement function calling and tool use training\n",
        "- Build code execution environment integration\n",
        "- Study agent frameworks and reasoning loops\n",
        "- Create simple calculator/search tool integration\n",
        "\n",
        "**Advanced Multimodal**\n",
        "- Study recent architectures like LLaVA, InstructBLIP\n",
        "- Implement video understanding basics\n",
        "- Explore audio integration (speech recognition/generation)\n",
        "\n",
        "### Phase 5: Production and Research (3-4 weeks)\n",
        "\n",
        "**Production Systems**\n",
        "- Learn vLLM for high-throughput serving\n",
        "- Implement model caching and batching strategies\n",
        "- Study continuous batching and request scheduling\n",
        "- Build API endpoints with proper error handling\n",
        "\n",
        "**Research Skills**\n",
        "- Reproduce recent paper (choose from Anthropic, OpenAI, Google)\n",
        "- Implement novel architectural component\n",
        "- Design and run ablation studies\n",
        "- Learn experiment tracking with Weights & Biases\n",
        "\n",
        "### Practical Projects Throughout\n",
        "\n",
        "**Project 1 (Phase 1-2):** Build a LLaMA-style model from scratch, train on a domain-specific dataset (legal, medical, code)\n",
        "\n",
        "**Project 2 (Phase 3):** Create an instruction-tuned chatbot with safety measures, deploy it with a web interface\n",
        "\n",
        "**Project 3 (Phase 4):** Build a multimodal model that can answer questions about images and execute code\n",
        "\n",
        "**Project 4 (Phase 5):** Research project - implement and evaluate a novel technique from recent papers\n",
        "\n",
        "### Essential Resources\n",
        "\n",
        "**Technical Papers to Study:**\n",
        "- \"Attention Is All You Need\" (review)\n",
        "- LLaMA, LLaMA 2 papers\n",
        "- InstructGPT, Constitutional AI\n",
        "- Flamingo, LLaVA papers\n",
        "- Recent Anthropic, OpenAI research\n",
        "\n",
        "**Key Repositories:**\n",
        "- nanoGPT (Karpathy) - study the clean implementations\n",
        "- Transformers library source code\n",
        "- LLaMA implementations (multiple on GitHub)\n",
        "- vLLM, text-generation-inference\n",
        "\n",
        "**Practical Skills:**\n",
        "- Master Weights & Biases for experiment tracking\n",
        "- Learn Hugging Face ecosystem deeply\n",
        "- Get comfortable with cloud platforms (AWS, GCP)\n",
        "- Study MLOps practices for model deployment\n",
        "\n",
        "### Success Metrics\n",
        "\n",
        "By the end, you should be able to:\n",
        "- Train instruction-following models from scratch\n",
        "- Implement and evaluate safety measures\n",
        "- Build multimodal applications\n",
        "- Deploy models efficiently in production\n",
        "- Read and implement recent research papers\n",
        "- Contribute to open-source LLM projects"
      ],
      "metadata": {
        "id": "q8p8ljXCbDKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r18u1jlEuZS3"
      },
      "outputs": [],
      "source": [
        "%pip install nbstripout\n",
        "# in terminal nbstripout Training-a-LLM-with-Shakespear.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dud_wjceiY1G"
      },
      "outputs": [],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnmWg2-2upR4"
      },
      "source": [
        "# Training of GPT2 style LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMFKmlVbjQI0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import os\n",
        "# import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import requests\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "import random\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM4zI7Phuw15"
      },
      "source": [
        "## GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDBAVJm-j3BQ"
      },
      "outputs": [],
      "source": [
        "# GPT2 Model\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, max_seq, rope=True):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            # add rotary position embedding on qk\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(self.head_dim, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.query_linear(query)\n",
        "        key = self.key_linear(key)\n",
        "        value = self.value_linear(value)\n",
        "        # split - allow computing for n heads at the same time.\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # optional\n",
        "        # RoPE for positional encoding\n",
        "        if self.rotary_embedding is not None:\n",
        "          query = self.rotary_embedding(query)\n",
        "          key = self.rotary_embedding(key)\n",
        "        value = value.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "        # attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim) # keep variant stable\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -float('inf'))\n",
        "        else:\n",
        "            # for decoder-style, add tril mask by default in both training and generating\n",
        "            scores = scores.masked_fill(torch.tril(torch.ones_like(scores, device=query.device)) == 0, -1e9)\n",
        "        # softmax\n",
        "        attention = F.softmax(scores, dim=-1) # b, n, s, s\n",
        "        # output - use contiguous to sort RAM to make view faster.\n",
        "        output = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # linear projection\n",
        "        return self.output_linear(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, max_seq, dropout=0.1, rope=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, heads, max_seq, rope)\n",
        "        # self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.feed_forward = SwiGLUFFN(d_model, d_ff, dropout) # use SwiGLU for gated activation.\n",
        "        # self.norm1 = nn.LayerNorm(d_model)\n",
        "        # self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm1 = nn.RMSNorm(d_model) # switch to RMSNorm(d_model) for stability\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        # Use pre-norm\n",
        "        # input -> norm -> attention -> residue -> norm -> ffn -> residue -> output\n",
        "        # attention\n",
        "        residue = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attention(x, x, x, mask)\n",
        "        x = self.dropout(x)\n",
        "        x = x + residue\n",
        "        # feed forward\n",
        "        residue = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + residue\n",
        "        return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout=0.1,rope=False):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq, d_model)\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(d_model, heads, d_ff, max_seq, dropout, rope) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size) # final project layer\n",
        "        self.apply(self._init_weights)\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # if mask is None, casual mask will added automatically.\n",
        "        token_emb = self.embedding(x)\n",
        "        pos_emb = self.position_embedding(torch.arange(x.size(1), device=x.device))\n",
        "        x = token_emb + pos_emb\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x, mask)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoPE - Rotary Position Embedding\n",
        "- add position into q and k.\n",
        "- In order to have dot product of q_i, k_j reflect relative position i-j, rotate original by θ_i,\n",
        "- treate x as complext number x = a+bi, rotate θ gives a (cosθ+sinθ) + b (cosθ-sinθ)i. split x into [x1, x2], after rotate [x1cos-x2sin, x1sin+x2cos]\n",
        "- add frequencies for different dimension. f = base^-2d_i/d_model"
      ],
      "metadata": {
        "id": "VqN2_Csp-tcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq, base=10000):\n",
        "        super(RotaryPositionalEmbedding, self).__init__()\n",
        "        self.d_model = d_model # this should be d_model // n_headers\n",
        "        self.max_seq = max_seq\n",
        "        self.base = base\n",
        "        # register buffer\n",
        "        self.register_buffer('frequencies', self._get_frequencies())\n",
        "        cos, sin = self._get_cos_sin(seq_len=self.max_seq)\n",
        "        self.register_buffer('cos', cos)\n",
        "        self.register_buffer('sin', sin)\n",
        "\n",
        "    def _get_frequencies(self):\n",
        "        # f = base^-2di/d_model\n",
        "        assert self.d_model % 2 == 0, \"d_model must be divisible by 2\"\n",
        "        frequencies = 1.0 / (self.base ** (torch.arange(0, self.d_model, 2).float() / self.d_model)) # d_model/2\n",
        "        return frequencies\n",
        "\n",
        "    def _get_cos_sin(self, seq_len=None, device=None):\n",
        "        pos_i = torch.arange(seq_len).unsqueeze(1) # s, 1\n",
        "        if device is not None:\n",
        "          pos_i = pos_i.to(device)\n",
        "        freqs = self.frequencies.unsqueeze(0) # 1, d/2\n",
        "        args = pos_i * freqs # s, d/2\n",
        "        cos = torch.cos(args).unsqueeze(0) # 1, s, d/2\n",
        "        sin = torch.sin(args).unsqueeze(0) # 1, s, d/2\n",
        "        return cos, sin\n",
        "\n",
        "    def _apply_rope(self, x, cos, sin):\n",
        "        # split x into x1, x2\n",
        "        x1, x2 = x.chunk(2, dim=-1) # b, s, d/2\n",
        "        assert x1.size() == x2.size()\n",
        "        # rotate x1, x2\n",
        "        x1 = x1 * cos - x2 * sin # b, s, d/2\n",
        "        x2 = x1 * sin + x2 * cos # b, s, d/2\n",
        "        # concatenate\n",
        "        return torch.cat([x1, x2], dim=-1) # b, s, d\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.size(2) == self.max_seq:\n",
        "            con, sin = self.cos, self.sin\n",
        "        else:\n",
        "            cos, sin = self._get_cos_sin(seq_len=x.size(2), device=x.device)\n",
        "        return self._apply_rope(x, con, sin)\n"
      ],
      "metadata": {
        "id": "CdcjhgTl_twG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSNorm\n",
        "- LayerNorm: (x-μ)/σ. Shape of μ [batch_size, seq_length, 1].\n",
        "- weighted parameters: α * (x-μ)/σ + β. σ can be zero, catestrophic cancellation, lead to numeric instability.\n",
        "- RMSNorm: x/RMS(x): γ * x/RMS(x). Easy computing and avoid precision loss; also better gradient flow, each one is independent not relying on μ.\n",
        "- LayerNorm(x) = LayerNorm(x + c1). absolute position information is not captured. position [1,2,3] will have the same as position [10,20,30].\n",
        "- μ is not your friends empirically vulnerable to outliers.\n",
        "- BatchNorm: (x-μ)/σ. Shape of μ [1, 1, d_model]. Not good for LLM b/c batch variability can be large and different between training and testing. Even tricky with flexible input seq_len\n"
      ],
      "metadata": {
        "id": "-xRKFlfaE3hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        return self.gamma * x / (torch.norm(x, dim=-1, keepdim=True) + self.eps)\n",
        "\n",
        "# Pytorch has RMSNorm nn.RMSNorm(x)"
      ],
      "metadata": {
        "id": "O7xxsGoQIfsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SwiGLU activation\n",
        "- Similar to ReLU but with learned threshold (not like zero in ReLU)\n",
        "- Better gradient flow (allow non-zero gradient for negatives; avoid dieing ReLU or die neurons)\n",
        "- dead neurons: relu(x) = 0 receive 0 gradient, and stay 0 forever.\n",
        "- 2010s thinking: \"Dead neurons are bad, but sparsity is good\"\n",
        "- 2020s thinking: \"We want sparsity but not permenant dead\""
      ],
      "metadata": {
        "id": "UdsCiNGsdtMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLUFFN(nn.Module):\n",
        "    # SiLU(xW) * (xV)\n",
        "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
        "        super(SwiGLUFFN, self).__init__()\n",
        "        if d_ff is None:\n",
        "          d_ff = int(d_model * 8/3)\n",
        "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.up_proj= nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = F.silu(self.gate_proj(x))\n",
        "        up = self.up_proj(x)\n",
        "        hidden = gate * up\n",
        "        down = self.down_proj(self.dropout(hidden))\n",
        "        return down\n"
      ],
      "metadata": {
        "id": "hq7X_qsThYv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Query Attention (MQA)\n",
        "- shared k,v matrix across all headers\n",
        "- much faster, lower performance reduce\n"
      ],
      "metadata": {
        "id": "T3syxUjRxOuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, max_seq, rope=True):\n",
        "        super(MultiQueryAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, self.head_dim) # only head_dim, shared across headers\n",
        "        self.wv = nn.Linear(d_model, self.head_dim) # only head_dim, shared across headers\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(self.head_dim, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.wq(query) # b,s,d*n\n",
        "        key = self.wk(key) # b,s,d\n",
        "        value = self.wv(value) # b,s,d\n",
        "        # split\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.unsqueeze(1).expand(-1, query.size(1), -1, -1)\n",
        "        value = value.unsqueeze(1).expand(-1, query.size(1), -1, -1)\n",
        "        # optional\n",
        "        # RoPE for positional encoding\n",
        "        if self.rotary_embedding is not None:\n",
        "            query = self.rotary_embedding(query)\n",
        "            key = self.rotary_embedding(key)\n",
        "        # attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # mask\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        else:\n",
        "            scores = scores.masked_fill(torch.tril(torch.ones_like(scores, device=query.device)) == 0, -1e9)\n",
        "        # softmax\n",
        "        attention = F.softmax(scores, dim=-1) # b, n, s, s\n",
        "        # output\n",
        "        output = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # linear projection\n",
        "        return self.output_linear(output)"
      ],
      "metadata": {
        "id": "e_6HlWUmxcFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouped Query Attention\n",
        "- Compromise between MQA and MHA.\n",
        "- Group some headers to share K, V"
      ],
      "metadata": {
        "id": "RB4W_RtJzOrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, kv_groups, max_seq, rope=True):\n",
        "        super(GroupedQueryAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.kv_groups = kv_groups\n",
        "        self.heads_per_group = heads // kv_groups\n",
        "        assert self.heads_per_group * kv_groups == heads, \"heads must be divisible by kv_groups\"\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.group_dim = self.head_dim * self.kv_groups\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, self.group_dim)\n",
        "        self.wv = nn.Linear(d_model, self.group_dim)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(self.head_dim, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.wq(query) # b,s,d*n\n",
        "        key = self.wk(key) # b,s,d*m\n",
        "        value = self.wv(value) # b,s,d*m\n",
        "        # split\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.view(batch_size, -1, self.kv_groups, self.head_dim).transpose(1, 2).repeat_interleave(self.heads_per_group, dim=1)\n",
        "        value = value.view(batch_size, -1, self.kv_groups, self.head_dim).transpose(1, 2).repeat_interleave(self.heads_per_group, dim=1)\n",
        "        # rope\n",
        "        if self.rotary_embedding is not None:\n",
        "            query = self.rotary_embedding(query)\n",
        "            key = self.rotary_embedding(key)\n",
        "        # score\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        # mask\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        else:\n",
        "            scores = scores.masked_fill(torch.tril(torch.ones_like(scores, device=query.device)) == 0, -1e9)\n",
        "        # softmax\n",
        "        attention = F.softmax(scores, dim=-1) # b, n, s, s\n",
        "        # output\n",
        "        output = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # linear projection\n",
        "        return self.output_linear(output)\n"
      ],
      "metadata": {
        "id": "EMSXDZaXzifY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FlashAttention\n",
        "- The key concept of FlashAttention is there is a memory movement cost (which is the bottleneck) between  fast RAM or L1/L2 Caches (SRAM, 100TB/s) and global RAM in CUDA GPU (HBM/DRAM, 2TB/s). This is similar to I/O cost.\n",
        "- Everytime, pytorch line (or more accurately, a cuda kernel) will materialize (similar to writing out to disk) the variable in the global RAM.\n",
        "- However, we don't have to do that, if we calculate everything in HRAM, and store the calculated variables in the HRAM for later kernel operation\n",
        "- That will require us to break down the large matrix calculation into blocked matrix calculation (b/c the HRAM is smaller than global RAM)\n",
        "- But you can't really implement this by change the pytorch code b/c pytorch will still materialize the blocked matrix calculation results (determined by its kernel function).\n",
        "- The easiest you can do is to use `scaled_dot_product_attention`, which do blocked kernel calculation under the hood.\n",
        "- You can also use Triton, which is a python way to implement cuda kernels. see [this example](https://arunjitha.medium.com/simplifying-cuda-kernels-with-triton-a-pythonic-approach-to-gpu-programming-79bb7121e974)"
      ],
      "metadata": {
        "id": "yZ5_o2AD4oyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "\n",
        "class FlashAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, max_seq, rope=True):\n",
        "        super(FlashAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(self.head_dim, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.query_linear(query)\n",
        "        key = self.key_linear(key)\n",
        "        value = self.value_linear(value)\n",
        "        # split\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "        # optional\n",
        "        # RoPE for positional encoding\n",
        "        if self.rotary_embedding is not None:\n",
        "            query = self.rotary_embedding(query)\n",
        "            key = self.rotary_embedding(key)\n",
        "        # attention with flash_attention implemented under the hood\n",
        "        with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
        "            scores = scaled_dot_product_attention(query, key, value, attn_mask=mask, enable_gqa=False)\n",
        "        # output\n",
        "        return self.output_linear(scores)\n"
      ],
      "metadata": {
        "id": "cRCydqNV6Cjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention with Sliding window and Sparse Attention\n",
        "- mainly used in long-context scenario\n",
        "- sparse attention usually include global attention + local attention\n",
        "- global attention can be random attention; first_n token attention and stride-based attention;\n",
        "- the implementation is to create various mask where mask[i,j] = 1 means i will attend to j.\n",
        "- different attention can be combined as hierachical ways to form a bigbird mask\n",
        "- there are other attention ways like hash bucket based or other row-wise and column-wise based. But the overall idea is similar."
      ],
      "metadata": {
        "id": "8A_t039n19sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_local_masks(seq_len, local_window_size):\n",
        "    masks = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "    masks.fill_diagonal_(1)\n",
        "    for i in range(seq_len):\n",
        "        # gpt always attend to tokens before i\n",
        "        start = max(0, i - local_window_size)\n",
        "        end = i\n",
        "        masks[i, start:end] = True\n",
        "    return masks.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "def create_global_first_n_masks(seq_len, first_n):\n",
        "    masks = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "    masks.fill_diagonal_(1)\n",
        "    for i in range(seq_len):\n",
        "        start = 0\n",
        "        end = min(first_n, i)\n",
        "        masks[i, start:end] = True\n",
        "    return masks.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "def create_global_stride_masks(seq_len, stride):\n",
        "    masks = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "    masks.fill_diagonal_(1)\n",
        "    for i in range(seq_len):\n",
        "        start = 0\n",
        "        end = i\n",
        "        masks[i, start:end:stride] = True\n",
        "    return masks.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "def create_global_random_masks(seq_len, prob=0.1, device='cpu', seed=42):\n",
        "    # fix seed to ensure training and generating are same\n",
        "    generator = torch.Generator(device=device)\n",
        "    generator.manual_seed(seed)\n",
        "    masks = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "    masks.fill_diagonal_(1)\n",
        "    for i in range(1, seq_len):\n",
        "        random_vals = torch.rand(i, generator=generator, device=device)\n",
        "        masks[i, :i] = random_vals < prob\n",
        "    return masks.unsqueeze(0).unsqueeze(0)"
      ],
      "metadata": {
        "id": "MI69M3Oz79bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight Tieing\n",
        "- The embedding layer and last projection layer share parameters.\n",
        "- Intuitively it make sense, it also saves the paremeters (d_model * vocabulary_size)\n",
        "- Empirically, Llama implemented in that way.\n",
        "- Two ways to implement, in GPT style, same parameter will receive gradient from two paths embedding and final fc layer; In llama, simply multiple embedding weight to get logits, no gradient flow in the last layer."
      ],
      "metadata": {
        "id": "-foJxtsXBbgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTwithWeightTier(GPT2):\n",
        "    def __init__(self, vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout=0.1, rope=False, weight_tie=False):\n",
        "        super(GPTwithWeightTier, self).__init__(vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout, rope)\n",
        "        if weight_tie:\n",
        "            assert self.embedding is not None, \"embedding layer must be defined\"\n",
        "            self.fc = nn.linear(d_model, vocab_size, bias=False) # it is a function\n",
        "            self.fc.weight = self.embedding.weight\n",
        "    def forward(self, x, mask=None):\n",
        "        return super(GPTwithWeightTier, self).forward(x, mask)"
      ],
      "metadata": {
        "id": "13BXojOzBvyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixture of Experts (MoE)\n",
        "- replace the FFN after attention\n",
        "- Original TransformerBlock: LayerNorm -> Attention -> Residue -> LayerNorm -> FFN -> NextBlock\n",
        "- MoEBlock: LayerNorm -> Attention -> Residue -> Layernomr -> MoE -> NextBlock\n",
        "- The idea is instead of using only one FFN, we want to use weighted sum of num_experts number of FFNs (could be smaller d_ff). But we don't want to calculate num_experts for each token to increase the computational burden by num_experts times. Instead, we used a gated mechanism to only activate topk based on a gate_activation function.\n"
      ],
      "metadata": {
        "id": "sYeBzccQLTLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleExpertFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(SingleExpertFFN, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class GatedFFN(nn.Module):\n",
        "    def __init__(self, d_model, top_k, num_experts):\n",
        "        super(GatedFFN, self).__init__()\n",
        "        self.gate_proj = nn.Linear(d_model, num_experts, bias=False)\n",
        "        self.top_k = top_k\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def _compute_load_balance_loss(self, gate_scores, top_k_indices):\n",
        "        gate_prob = F.softmax(gate_scores, dim=-1) # b, s, e\n",
        "        mean_gate_prob = gate_prob.mean(dim=[0,1]) # e\n",
        "        tokens_per_experts = torch.zeros(self.num_experts, device=gate_scores.device) # e\n",
        "        for i in range(self.num_experts):\n",
        "            tokens_per_experts[i] = (top_k_indices == i).float().sum()\n",
        "        # normalize\n",
        "        total_tokens = gate_scores.size(0) * gate_scores.size(1) * self.top_k # b*s*k\n",
        "        token_usage_gate_prob = tokens_per_experts / total_tokens # e, sum(e) = 1\n",
        "        # compute loss - penalize deviations from uniform expert usage.\n",
        "        # similar to entropy but considered both continues prob (mean_gate_prob) and cut-off prob (token_usage)\n",
        "        return torch.sum(token_usage_gate_prob * mean_gate_prob) * self.num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate_scores = self.gate_proj(x) # b, s, e\n",
        "        top_k_scores, top_k_indices = torch.topk(gate_scores, self.top_k, dim=-1) # b, s, k\n",
        "        top_k_gate_weight = F.softmax(top_k_scores, dim=-1) # b, s, k\n",
        "\n",
        "        if self.training:\n",
        "            load_balance_loss = self._compute_load_balance_loss(gate_scores, top_k_indices)\n",
        "\n",
        "        else:\n",
        "            load_balance_loss = torch.tensor(0.0, device=x.device)\n",
        "\n",
        "        return top_k_gate_weight, top_k_indices, load_balance_loss\n",
        "\n",
        "class MoEFFN(nn.Module):\n",
        "    def __init__(self, d_model, top_k, d_ff, num_experts, dropout=0.1):\n",
        "        super(MoEFFN, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.expert_ffns = nn.ModuleList([SingleExpertFFN(d_model, d_ff, dropout) for _ in range(num_experts)])\n",
        "        self.gate_activation = GatedFFN(d_model, top_k, num_experts)\n",
        "        self.top_k = top_k\n",
        "    def forward(self, x):\n",
        "        top_k_gate_weight, top_k_indices, load_balance_loss = self.gate_activation(x) # b,s,k\n",
        "        flattened_x = x.view(-1, x.size(-1)) # b*s, d -> b*s tokens, each token has own top_k gate and prob\n",
        "        flattened_top_k_gate_weight = top_k_gate_weight.view(-1, top_k_gate_weight.size(-1)) # b*s, k\n",
        "        flattened_top_k_indices = top_k_indices.view(-1, top_k_indices.size(-1)) # b*s, k\n",
        "        weighted_expert_outputs = torch.zeros_like(flattened_x) # b*s, d\n",
        "        for j in range(self.top_k):\n",
        "          # get current expert idx\n",
        "          top_k_expert_idx = flattened_top_k_indices[:, j] # b*s\n",
        "          top_k_expert_weight = flattened_top_k_gate_weight[:, j] # b*s\n",
        "          # process current j position for each token\n",
        "          for i in range(self.num_experts):\n",
        "              expert_mask = (top_k_expert_idx == i) # b*s\n",
        "              if expert_mask.any():\n",
        "                # only process expert with at least one tokens\n",
        "                expert_input = flattened_x[expert_mask] # ?, d\n",
        "                expert_output = self.expert_ffns[i](expert_input) # ?, d\n",
        "                expert_weight = top_k_expert_weight[expert_mask].unsqueeze(-1) # ?, 1\n",
        "                weighted_expert_outputs[expert_mask] += expert_weight * expert_output # ?, 1\n",
        "\n",
        "        return weighted_expert_outputs.view(x.size(0), x.size(1), -1),  load_balance_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "lIT6kUczM9LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAg22vZyu0jB"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi9LfGm0mgrz"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.seq = self.tokenizer.encode(data)\n",
        "        self.seq_num = len(self.seq) - max_length - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seq_num\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = self.seq[idx:idx+self.max_length]\n",
        "        target = self.seq[idx+1:idx+self.max_length+1]\n",
        "        return torch.tensor(input, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhwqEfFfu6Y3"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0yPy2WTniTC"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "class Trainer:\n",
        "    def __init__(self, model, device, learning_rate = 1e-3, weight_decay = 0.01, warmup_steps = 1000, max_steps = 10000, gradient_accumulation_steps = 1, grad_clip = 1.0, save_dir = \"./checkpoints\"):\n",
        "        self.model = model\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.scheduler = self._get_scheduler(warmup_steps, max_steps)\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.grad_clip = grad_clip\n",
        "        self.save_dir = save_dir\n",
        "        self.global_step = 0\n",
        "        self.best_loss = float('inf')\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    def _get_scheduler(self, warmup_steps, max_steps):\n",
        "        def lr_lambda(current_step):\n",
        "            if current_step < warmup_steps:\n",
        "                return float(current_step) / float(max(1, warmup_steps))\n",
        "            # Cosine decay after warmup\n",
        "            progress = float(current_step - warmup_steps) / float(max(1, max_steps - warmup_steps))\n",
        "            return max(0.0, 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(progress))))\n",
        "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "\n",
        "    def train(self, train_dataloader, validate_dateloader, epochs):\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            total_loss = 0\n",
        "            pb = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                loss = loss / self.gradient_accumulation_steps if self.gradient_accumulation_steps > 1 else loss\n",
        "                loss.backward()\n",
        "                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
        "                    self.optimizer.step()\n",
        "                    self.scheduler.step()\n",
        "                    self.global_step += 1\n",
        "                # update pb for loss\n",
        "                total_loss += loss.item() * self.gradient_accumulation_steps\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / (batch_idx + 1):4f}\")\n",
        "            # call validate\n",
        "            self.validate(validate_dateloader)\n",
        "        print(f\"Training finished\")\n",
        "        return 1\n",
        "\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        pb = tqdm(dataloader, desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                total_loss += loss.item()\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / (batch_idx + 1):4f}\")\n",
        "\n",
        "        if (total_loss / (batch_idx + 1)) < self.best_loss:\n",
        "            self.best_loss = (total_loss / (batch_idx + 1))\n",
        "            self.save_checkpoint()\n",
        "\n",
        "        return 1\n",
        "\n",
        "    def save_checkpoint(self, filename='best_model.pt'):\n",
        "        checkpoint_path = os.path.join(self.save_dir, filename)\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'global_step': self.global_step,\n",
        "            'best_loss': self.best_loss\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    def load_checkpoint(self, filename='best_model.pt'):\n",
        "        checkpoint_path = os.path.join(self.save_dir, filename)\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        self.model_config = checkpoint['model_config']\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.global_step = checkpoint['global_step']\n",
        "        self.best_loss = checkpoint['best_loss']\n",
        "        print(f\"Checkpoint loaded from {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DDP (Distributed Data Parallel)\n",
        "- across multiple nodes (x gpu per nodes)\n",
        "- each node contains same dataset replica, model and environment\n",
        "- node know each other through world size and master node\n",
        "- each iteration update same model (model wrapped using DDP), same parameter, but different sampled dataset (from a same dataset) across nodes\n",
        "- back propogation (loss) auto syncronized (all reduced) through torch.distributed\n",
        "- able to process larger batch size (128 in one node with 2 gpu -> 128 * 8 with 4 nodes with 2 gpus), improve training efficiency\n",
        "- only save one copy and print one loss b/c they are all syned across nodes (rank = 0)\n",
        "- support both gpu (GLOO) and cpu (NCCL) training\n",
        "- use either torchrun (production, better fault tolerance) or multiprocessing.spawn\n"
      ],
      "metadata": {
        "id": "H7I8PSimVcdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "\n",
        "class DDPTrainer:\n",
        "    def __init__(self, model, world_size,, learning_rate = 1e-3, weight_decay = 0.01, warmup_steps = 1000, max_steps = 10000, gradient_accumulation_steps = 1, grad_clip = 1.0, save_dir = \"./checkpoints\"):\n",
        "        self.model = model\n",
        "        self.world_size = world_size\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.grad_clip = grad_clip\n",
        "        self.save_dir = save_dir\n",
        "        self.global_step = 0\n",
        "        self.best_loss = float('inf')\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_setup(rank, world_size, backend=\"nccl\"):\n",
        "        # nccl for gpu; gloo for cpu\n",
        "        os.environ['MASTER_ADDR'] = 'localhost'\n",
        "        os.environ['MASTER_PORT'] = '12355'\n",
        "        torch.cuda.set_device(rank)\n",
        "        dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def _cleanup():\n",
        "        dist.destroy_process_group()\n",
        "\n",
        "    def _get_scheduler(self, warmup_steps, max_steps):\n",
        "        def lr_lambda(current_step):\n",
        "            if current_step < warmup_steps:\n",
        "                return float(current_step) / float(max(1, warmup_steps))\n",
        "            # Cosine decay after warmup\n",
        "            progress = float(current_step - warmup_steps) / float(max(1, max_steps - warmup_steps))\n",
        "            return max(0.0, 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(progress))))\n",
        "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "\n",
        "    def train_master(self, train_dataset, val_dataloader, epochs, batch_size):\n",
        "        # master to spawn multiprocess\n",
        "        mp.spawn(self.train_worker, args=(train_dataset, val_dataloader, epochs, batch_size), nprocs=self.world_size, join=True)\n",
        "        return 1\n",
        "\n",
        "\n",
        "    def train_worker(self, rank, train_dataset, val_dataloader, epochs, batch_size):\n",
        "\n",
        "        # init progress group\n",
        "        self._init_setup(rank, self.world_size)\n",
        "\n",
        "        self.device = torch.device(f'cuda:{rank}')\n",
        "\n",
        "        # init model for this worker\n",
        "        self.model = self.model.to(self.device)\n",
        "        model = DDP(self.model, device_ids=[rank], output_device=rank)\n",
        "        model.train()\n",
        "\n",
        "        # Initialize optimizer, scheduler, and criterion for this worker\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        self.scheduler = self._get_scheduler(self.optimizer, self.warmup_steps, self.max_steps)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        #  load dataset - the entire data copy is needed in each node\n",
        "        train_dataset_sampler = DistributedSampler(self.train_dataset, num_replicas=self.world_size, rank=rank)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_dataset_sampler, pin_memory=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Set epoch for sampler to ensure different shuffling each epoch\n",
        "            train_sampler.set_epoch(epoch)\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            if rank == 0:\n",
        "                pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "            else:\n",
        "                pbar = train_dataloader\n",
        "            for batch_idx, (data, target) in enumerate(pbar):\n",
        "                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                loss = loss / self.gradient_accumulation_steps if self.gradient_accumulation_steps > 1 else loss\n",
        "                loss.backward()\n",
        "                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
        "                    self.optimizer.step()\n",
        "                    self.scheduler.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    self.global_step += 1\n",
        "                # update pb for loss\n",
        "                total_loss += loss.item() * self.gradient_accumulation_steps\n",
        "                num_batches += 1\n",
        "                # Update progress bar\n",
        "                if batch_idx % 100 == 0 and rank == 0: # only print in node 0.\n",
        "                    average_loss = total_loss / num_batches\n",
        "                    pbar.set_postfix(loss=f\"{average_loss:.4f}\", step=self.global_step)\n",
        "\n",
        "            # call validate\n",
        "            if rank == 0:\n",
        "              self.validate(model, validate_dateloader)\n",
        "\n",
        "        self._cleanup()\n",
        "        return 1\n",
        "\n",
        "    def validate(self, model, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        number_batches = 0\n",
        "\n",
        "        pb = tqdm(dataloader, desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / num_batches:4f}\")\n",
        "\n",
        "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "\n",
        "        if avg_loss < self.best_loss:\n",
        "            self.best_loss = avg_loss\n",
        "            self.save_checkpoint(model, loss)\n",
        "\n",
        "        return 1\n",
        "\n",
        "    def save_checkpoint(self, model, loss):\n",
        "        checkpoint_path = os.path.join(self.save_dir, 'best_model.pt')\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'global_step': self.global_step,\n",
        "            'best_loss': self.best_loss\n",
        "            'loss': loss\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    world_size = 4\n",
        "    trainer = DDPTrainer(model, world_size)\n",
        "    train_dataset = TextDataset(train_data[1:10000], tokenizer, model_config['max_seq'])\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
        "    epochs = 100\n",
        "    trainer.train_master(train_dataset, val_dataloader, epochs)\n"
      ],
      "metadata": {
        "id": "Og_HBFiQW_Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Parallel\n",
        "- DDP is faster but the bottleneck in modern LLM is GPU RAM.\n",
        "- What if the model and its parameters can not fit inside one GPU?\n",
        "- We have to parallel and share models parameters across GPUs.\n",
        "- Column parallel for a linear layer [int, out] -> [int, out/number_of_gpus]\n",
        "- Row parallel for a linear layer [int, out] -> [int/number_of_gpus, out]\n",
        "- use `dist.all_reduce`, `dist_all_gather` to communicate across devices"
      ],
      "metadata": {
        "id": "aN4u5LQYl97N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "\n",
        "class ColumnParallelLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank, world_size, gather_output=True):\n",
        "        super(ShardedLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.world_size = world_size\n",
        "        self.rank = rank\n",
        "        self.local_out_features = out_features // world_size\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, self.local_out_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.local_out_features))\n",
        "        self.gather_output = gather_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        local_output = F.linear(x, self.weight.t(), self.bias) # b, s, d // ws\n",
        "        if not self.gather_output:\n",
        "            return local_output\n",
        "        output_list = [torch.zeros_like(local_output) for _ in range(self.world_size)]\n",
        "        dist.all_gather(output_list, local_output) # b, s, d // ws\n",
        "        # cat\n",
        "        output = torch.cat(output_list, dim=-1)\n",
        "        return output\n",
        "\n",
        "def train_workder(rank, world_size, in_features, out_features):\n",
        "    # setup env. etc.\n",
        "\n",
        "    device = torch.device(f'cuda:{rank}')\n",
        "    model = ShardedLinear(in_features, out_features, rank, world_size).to(device)\n",
        "    model.train()\n",
        "\n",
        "    x = torch.randn(10, in_features).to(device)\n",
        "    y = model(x)\n",
        "    # loss update\n",
        "\n",
        "    # clean up\n",
        "\n",
        "\n",
        "\n",
        "# training example\n",
        "# init dist setup ,etc.\n",
        "in_features = 1024\n",
        "out_features = 2048\n",
        "rank = 0\n",
        "world_size = 4\n",
        "mp.spawn(train_workder, args=(world_size,in_features, out_features), nprocs=world_size, join=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "-Qn4b7P1ms0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ZeRO (Zero Redundancy Optimizer)\n",
        "- Optimizer stores parameters, gradient and states therefore cost lots of memory\n",
        "- For SGD, the state is just learning rate, so it is cheap. But in AdamW, there is m (first momentum) and v (second momentum) per parameter, so the state memory is 2*parameter - very expensive.\n",
        "- ZeRO is to shard state.\n",
        "- FSDP - shard state, gradient, and parameters. Reduces memory mostly but adds coordination during step().\n",
        "- DeepSpeed - shard state and gradient (middle ground)"
      ],
      "metadata": {
        "id": "8rvOkwdNz5mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distribute.optim import ZeroRedundancyOptimizer\n",
        "\n",
        "class ZeROTrainer(Trainer):\n",
        "    def __init__(self, model, device, learning_rate = 1e-3, weight_decay = 0.01, warmup_steps = 1000, max_steps = 10000, gradient_accumulation_steps = 1, grad_clip = 1.0):\n",
        "        super(ZeROTrainer, self).__init__(model, device, learning_rate, weight_decay, warmup_steps, max_steps, gradient_accumulation_steps, grad_clip)\n",
        "\n",
        "    # overwrite train_worker with ZeroRedundancyOptimizer\n",
        "    def train_worker(self, rank, train_dataset, val_dataloader, epochs, batch_size):\n",
        "      self.optimizer = ZeroRedundancyOptimizer(self.optimizer, self.model.parameters())\n",
        "\n"
      ],
      "metadata": {
        "id": "hpBRok7-1XFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixed Precision\n",
        "- using float16 instead of fp32 to save memory and accelerate operation\n",
        "- NVIDIA tensor core can handle matrix multiplication with fp16 much faster\n",
        "- add operation using fp32 to maintain numerical stability, multiplication using fp16 to make it faster\n",
        "- using torch autocaster context to make it stable\n",
        "- other format - fp32 (no tensor core), fp16, bf16, tf32, etc."
      ],
      "metadata": {
        "id": "FLHEcQWFWgqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MixedPrecisionTrainer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.scaler = GradScaler()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters())\n",
        "        assert torch.cuda.is_available(), \"CUDA is not available\"\n",
        "        self.device = torch.device(\"cuda\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, train_dataloader, epochs):\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            total_loss = 0\n",
        "            pb = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                with autocast(): # cast to fp16 for matmul operations\n",
        "                    output = self.model(data)\n",
        "                    loss = self.criterion(output, target)\n",
        "                self.scaler.scale(loss).backward() # scale the number by a constant to avoid fp16 underflow\n",
        "                self.scaler.step(self.optimizer) # unscale gradients before applying them\n",
        "                self.scaler.update() # adjust scale factor\n",
        "                self.optimizer.zero_grad()\n",
        "                total_loss += loss.item()\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / (batch_idx + 1):4f}\")\n",
        "\n",
        "            # validate\n",
        "            self.validate(validate_dataloader)\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        pb = tqdm(dataloader, desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                with autocast(): # ops (e.g.matmul) using FP16, ops (e.g. softmax) using FP32\n",
        "                    output = self.model(data)\n",
        "                    loss = self.criterion(output, target)\n",
        "                    total_loss += loss.item()"
      ],
      "metadata": {
        "id": "oqg6eRZRW7da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Checkpoint\n",
        "- During forward pass, activation/output/x of each layers will be calculated and stored, this is needed to calculate gradient in the backward. This will need lots of GPU RAM.\n",
        "- Solution, we don't store all x. We only store some x in the GPU RAM, and calculate the other x (e.g. x = relu(x), only store the x passed to relu) again when it is needed in backward pass.\n",
        "- trade-off between RAM and computing time\n",
        "- set preserve_rng_state = True to allow same mask inside those discarded hidden layers"
      ],
      "metadata": {
        "id": "p8LKvk4VWDxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class BigNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, layer_num, use_gc = True):\n",
        "        super(BigNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.blocks = nn.ModuleList([SimpleNN(hidden_size, hidden_size, hidden_size) for _ in range(layer_num)])\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.use_gc = use_gc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        for block in self.blocks:\n",
        "            if self.use_gc:\n",
        "                # set preserve_rng_state = True if drop out in block\n",
        "                x = checkpoint(block, x, use_reentrant=False, preserve_rng_state=False) # don't store the hidden layer output in simpleNN\n",
        "            else:\n",
        "                x = block(x)\n",
        "        return x\n",
        "\n",
        "# A minimal training example\n",
        "def run_one_batch(use_gc):\n",
        "    device = \"cuda\"\n",
        "    model = Model(size=4096, depth=12, use_gc=use_gc).to(device)\n",
        "    data = torch.randn(32, 4096, device=device, requires_grad=True)\n",
        "    target = torch.randn(32, 4096, device=device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # check memory\n",
        "    max_mem = torch.cuda.max_memory_allocated(device) / 1e6  # MB\n",
        "    return max_mem\n",
        "\n",
        "max_mem_gc = run_one_batch(True)\n",
        "max_mem_no_gc = run_one_batch(False)\n",
        "print(f\"Max memory with gradient checkpointing: {max_mem_gc:.2f} MB\")\n",
        "print(f\"Max memory without gradient checkpointing: {max_mem_no_gc:.2f} MB\")"
      ],
      "metadata": {
        "id": "3LXXM9DGXbsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP7MOGUQu9-a"
      },
      "source": [
        "## Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4pMf-ff2nxa"
      },
      "outputs": [],
      "source": [
        "class TextGenerator:\n",
        "    def __init__(self, model, tokenizer, device, max_length=100, eos_token=None):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        if eos_token is None:\n",
        "            self.eos_token = self.tokenizer.eot_token\n",
        "        else:\n",
        "            self.eos_token = eos_token\n",
        "\n",
        "    def generate(self, prompt, num_samples=1, temperature=1.0, top_k=50):\n",
        "        tokens = self.tokenizer.encode(prompt)\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0) # 0, s\n",
        "        for _ in range(num_samples):\n",
        "            for _ in range(self.max_length - len(tokens)):\n",
        "                logits = self.model(tokens)\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                filtered_logits = logits.topk(top_k)[0]\n",
        "                probs = F.softmax(filtered_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                if next_token.item() == self.eos_token:\n",
        "                    break\n",
        "            generated_text = self.tokenizer.decode(tokens[0].tolist())\n",
        "            print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oU6EZVWvDJq"
      },
      "source": [
        "## Training Shakespear GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOD1VLrkuN6p"
      },
      "outputs": [],
      "source": [
        "# define all training parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# get gpt2 tokenizer and vocab_size\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab\n",
        "# model parameters\n",
        "model_config = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"d_model\": 128,\n",
        "    \"max_seq\": 128,\n",
        "    \"n_layers\": 4,\n",
        "    \"heads\": 4,\n",
        "    \"d_ff\": 256,\n",
        "    \"dropout\": 0.1,\n",
        "    \"rope\": True\n",
        "}\n",
        "# training parameters\n",
        "batch_size=2\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 1000\n",
        "max_steps = 10000\n",
        "gradient_accumulation_steps = 5\n",
        "grad_clip = 1.0\n",
        "epochs = 2\n",
        "# train and val data\n",
        "if os.path.exists(\"data/t8.shakespeare.txt\"):\n",
        "    # if the file exists, read from it\n",
        "    with open(\"data/t8.shakespeare.txt\", \"r\") as f:\n",
        "        data = f.read()\n",
        "else:\n",
        "    shakespear_content_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "    # read the content from the URL\n",
        "    data = requests.get(shakespear_content_url).text\n",
        "    # save to content to local file\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "    with open(\"data/t8.shakespeare.txt\", \"w\") as f:\n",
        "        f.write(data)\n",
        "train_data, val_data = data[:int(len(data)*0.8)], data[int(len(data)*0.8):]\n",
        "train_dataloader = DataLoader(TextDataset(train_data[1:10000], tokenizer, model_config['max_seq']), batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(TextDataset(val_data[1:1000], tokenizer, model_config['max_seq']), batch_size=batch_size, shuffle=True)\n",
        "model = GPT2(**model_config)\n",
        "# init model and trainer\n",
        "trainer = Trainer(model, device, learning_rate, weight_decay, warmup_steps, max_steps, gradient_accumulation_steps, grad_clip, \"checkpoints\")\n",
        "# load model if available\n",
        "if os.path.exists(\"checkpoints/best_model.pt\"):\n",
        "    try:\n",
        "      trainer.load_checkpoint()\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "# start training\n",
        "trainer.train(train_dataloader, val_dataloader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUm-ifzevLy2"
      },
      "source": [
        "## Inference on Trained GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWbDPn2JwHu6"
      },
      "outputs": [],
      "source": [
        "# generate\n",
        "# load bst model\n",
        "model = GPT2(**model_config)\n",
        "model.load_state_dict(torch.load(\"checkpoints/best_model.pt\")['model_state_dict'])\n",
        "eos_token = tokenizer.eot_token\n",
        "text_generator = TextGenerator(model, tokenizer, device, max_length=100, eos_token=eos_token)\n",
        "prompt = \"he is a \"\n",
        "text_generator.generate(prompt, num_samples=1, temperature=1.0, top_k=50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "12ezKk_kmj9oUbRsIM6-sAmzGxxSjZ0QR",
      "authorship_tag": "ABX9TyMyRfpoZRTDqJyNSm1y+5SN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}