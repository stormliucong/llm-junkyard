{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stormliucong/llm-junkyard/blob/main/Training_a_LLM_with_Shakespear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSqHGcJE3ctk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/gpt2\"\n",
        "import os\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "%cd $path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured roadmap to modern LLM/generative AI:\n",
        "\n",
        "### Phase 1: Advanced Architectures (4-6 weeks)\n",
        "\n",
        "**Modern Transformer Variants**\n",
        "- Implement RoPE (Rotary Position Embedding) from scratch - used in LLaMA, PaLM\n",
        "- Build RMSNorm instead of LayerNorm - more stable training\n",
        "- Code SwiGLU activation function - better than ReLU/GELU\n",
        "- Practice: Modify your GPT-2 to use these components\n",
        "\n",
        "**Attention Mechanisms**\n",
        "- Implement Multi-Query Attention (MQA) and Grouped-Query Attention (GQA)\n",
        "- Build FlashAttention-style memory-efficient attention\n",
        "- Add attention variants: Sliding window, sparse attention patterns\n",
        "- Practice: Compare memory usage and speed vs standard attention\n",
        "\n",
        "**Advanced Architectures**\n",
        "- Study and implement key components of LLaMA architecture\n",
        "- Build a simple Mixture of Experts (MoE) layer\n",
        "- Understand and code basic retrieval-augmented generation\n",
        "\n",
        "### Phase 2: Scaling and Efficiency (4-5 weeks)\n",
        "\n",
        "**Distributed Training**\n",
        "- Learn PyTorch DDP (DistributedDataParallel)\n",
        "- Implement gradient accumulation and mixed precision training\n",
        "- Study DeepSpeed ZeRO stages - implement ZeRO-1\n",
        "- Practice: Scale your model training across multiple GPUs\n",
        "\n",
        "**Memory Optimization**\n",
        "- Implement gradient checkpointing\n",
        "- Study and code model sharding techniques\n",
        "- Learn about activation recomputation\n",
        "- Practice: Train larger models with limited memory\n",
        "\n",
        "**Inference Optimization**\n",
        "- Implement KV-cache for faster autoregressive generation\n",
        "- Build basic quantization (INT8, INT4)\n",
        "- Code speculative decoding with a smaller draft model\n",
        "- Study ONNX conversion and TensorRT optimization\n",
        "\n",
        "### Phase 3: Training Methodologies (3-4 weeks)\n",
        "\n",
        "**Advanced Training Techniques**\n",
        "- Implement curriculum learning and data scheduling\n",
        "- Study and code basic RLHF pipeline with reward models\n",
        "- Learn about instruction tuning and chat formatting\n",
        "- Practice: Fine-tune a model for instruction following\n",
        "\n",
        "**Safety and Alignment**\n",
        "- Implement Constitutional AI training loop\n",
        "- Build rejection sampling and best-of-N training\n",
        "- Study red-teaming and safety evaluation methods\n",
        "- Create your own harmlessness dataset and training\n",
        "\n",
        "### Phase 4: Multimodal and Tool Use (4-5 weeks)\n",
        "\n",
        "**Vision-Language Models**\n",
        "- Study CLIP architecture and implement vision encoder\n",
        "- Build simple vision-language connector (linear projection)\n",
        "- Implement basic VQA (Visual Question Answering) model\n",
        "- Practice: Train on image-text pairs\n",
        "\n",
        "**Tool Integration**\n",
        "- Implement function calling and tool use training\n",
        "- Build code execution environment integration\n",
        "- Study agent frameworks and reasoning loops\n",
        "- Create simple calculator/search tool integration\n",
        "\n",
        "**Advanced Multimodal**\n",
        "- Study recent architectures like LLaVA, InstructBLIP\n",
        "- Implement video understanding basics\n",
        "- Explore audio integration (speech recognition/generation)\n",
        "\n",
        "### Phase 5: Production and Research (3-4 weeks)\n",
        "\n",
        "**Production Systems**\n",
        "- Learn vLLM for high-throughput serving\n",
        "- Implement model caching and batching strategies\n",
        "- Study continuous batching and request scheduling\n",
        "- Build API endpoints with proper error handling\n",
        "\n",
        "**Research Skills**\n",
        "- Reproduce recent paper (choose from Anthropic, OpenAI, Google)\n",
        "- Implement novel architectural component\n",
        "- Design and run ablation studies\n",
        "- Learn experiment tracking with Weights & Biases\n",
        "\n",
        "### Practical Projects Throughout\n",
        "\n",
        "**Project 1 (Phase 1-2):** Build a LLaMA-style model from scratch, train on a domain-specific dataset (legal, medical, code)\n",
        "\n",
        "**Project 2 (Phase 3):** Create an instruction-tuned chatbot with safety measures, deploy it with a web interface\n",
        "\n",
        "**Project 3 (Phase 4):** Build a multimodal model that can answer questions about images and execute code\n",
        "\n",
        "**Project 4 (Phase 5):** Research project - implement and evaluate a novel technique from recent papers\n",
        "\n",
        "### Essential Resources\n",
        "\n",
        "**Technical Papers to Study:**\n",
        "- \"Attention Is All You Need\" (review)\n",
        "- LLaMA, LLaMA 2 papers\n",
        "- InstructGPT, Constitutional AI\n",
        "- Flamingo, LLaVA papers\n",
        "- Recent Anthropic, OpenAI research\n",
        "\n",
        "**Key Repositories:**\n",
        "- nanoGPT (Karpathy) - study the clean implementations\n",
        "- Transformers library source code\n",
        "- LLaMA implementations (multiple on GitHub)\n",
        "- vLLM, text-generation-inference\n",
        "\n",
        "**Practical Skills:**\n",
        "- Master Weights & Biases for experiment tracking\n",
        "- Learn Hugging Face ecosystem deeply\n",
        "- Get comfortable with cloud platforms (AWS, GCP)\n",
        "- Study MLOps practices for model deployment\n",
        "\n",
        "### Success Metrics\n",
        "\n",
        "By the end, you should be able to:\n",
        "- Train instruction-following models from scratch\n",
        "- Implement and evaluate safety measures\n",
        "- Build multimodal applications\n",
        "- Deploy models efficiently in production\n",
        "- Read and implement recent research papers\n",
        "- Contribute to open-source LLM projects"
      ],
      "metadata": {
        "id": "q8p8ljXCbDKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r18u1jlEuZS3"
      },
      "outputs": [],
      "source": [
        "%pip install nbstripout\n",
        "# in terminal nbstripout Training-a-LLM-with-Shakespear.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dud_wjceiY1G"
      },
      "outputs": [],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnmWg2-2upR4"
      },
      "source": [
        "# Training of GPT2 style LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMFKmlVbjQI0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import os\n",
        "# import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import requests\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "import random\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM4zI7Phuw15"
      },
      "source": [
        "## GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDBAVJm-j3BQ"
      },
      "outputs": [],
      "source": [
        "# GPT2 Model\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, max_seq, rope=True):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            # add rotary position embedding on qk\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(self.head_dim, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.query_linear(query)\n",
        "        key = self.key_linear(key)\n",
        "        value = self.value_linear(value)\n",
        "        # split - allow computing for n heads at the same time.\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # optional\n",
        "        # RoPE for positional encoding\n",
        "        if self.rotary_embedding is not None:\n",
        "          query = self.rotary_embedding(query)\n",
        "          key = self.rotary_embedding(key)\n",
        "        value = value.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "        # attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim) # keep variant stable\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -float('inf'))\n",
        "        else:\n",
        "            # for decoder-style, add tril mask by default in both training and generating\n",
        "            scores = scores.masked_fill(torch.tril(torch.ones_like(scores, device=query.device)) == 0, -1e9)\n",
        "        # softmax\n",
        "        attention = F.softmax(scores, dim=-1) # b, n, s, s\n",
        "        # output - use contiguous to sort RAM to make view faster.\n",
        "        output = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # linear projection\n",
        "        return self.output_linear(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, max_seq, dropout=0.1, rope=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, heads, max_seq, rope)\n",
        "        # self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.feed_forward = SwiGLUFFN(d_model, d_ff, dropout) # use SwiGLU for gated activation.\n",
        "        # self.norm1 = nn.LayerNorm(d_model)\n",
        "        # self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm1 = nn.RMSNorm(d_model) # switch to RMSNorm(d_model) for stability\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        # Use pre-norm\n",
        "        # input -> norm -> attention -> residue -> norm -> ffn -> residue -> output\n",
        "        # attention\n",
        "        residue = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attention(x, x, x, mask)\n",
        "        x = self.dropout(x)\n",
        "        x = x + residue\n",
        "        # feed forward\n",
        "        residue = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + residue\n",
        "        return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout=0.1,rope=False):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq, d_model)\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(d_model, heads, d_ff, max_seq, dropout, rope) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size) # final project layer\n",
        "        self.apply(self._init_weights)\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # if mask is None, casual mask will added automatically.\n",
        "        token_emb = self.embedding(x)\n",
        "        pos_emb = self.position_embedding(torch.arange(x.size(1), device=x.device))\n",
        "        x = token_emb + pos_emb\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x, mask)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoPE - Rotary Position Embedding\n",
        "- add position into q and k.\n",
        "- In order to have dot product of q_i, k_j reflect relative position i-j, rotate original by θ_i,\n",
        "- treate x as complext number x = a+bi, rotate θ gives a (cosθ+sinθ) + b (cosθ-sinθ)i. split x into [x1, x2], after rotate [x1cos-x2sin, x1sin+x2cos]\n",
        "- add frequencies for different dimension. f = base^-2d_i/d_model"
      ],
      "metadata": {
        "id": "VqN2_Csp-tcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq, base=10000):\n",
        "        super(RotaryPositionalEmbedding, self).__init__()\n",
        "        self.d_model = d_model # this should be d_model // n_headers\n",
        "        self.max_seq = max_seq\n",
        "        self.base = base\n",
        "        # register buffer\n",
        "        self.register_buffer('frequencies', self._get_frequencies())\n",
        "        cos, sin = self._get_cos_sin(seq_len=self.max_seq)\n",
        "        self.register_buffer('cos', cos)\n",
        "        self.register_buffer('sin', sin)\n",
        "\n",
        "    def _get_frequencies(self):\n",
        "        # f = base^-2di/d_model\n",
        "        assert self.d_model % 2 == 0, \"d_model must be divisible by 2\"\n",
        "        frequencies = 1.0 / (self.base ** (torch.arange(0, self.d_model, 2).float() / self.d_model)) # d_model/2\n",
        "        return frequencies\n",
        "\n",
        "    def _get_cos_sin(self, seq_len=None, device=None):\n",
        "        pos_i = torch.arange(seq_len).unsqueeze(1) # s, 1\n",
        "        if device is not None:\n",
        "          pos_i = pos_i.to(device)\n",
        "        freqs = self.frequencies.unsqueeze(0) # 1, d/2\n",
        "        args = pos_i * freqs # s, d/2\n",
        "        cos = torch.cos(args).unsqueeze(0) # 1, s, d/2\n",
        "        sin = torch.sin(args).unsqueeze(0) # 1, s, d/2\n",
        "        return cos, sin\n",
        "\n",
        "    def _apply_rope(self, x, cos, sin):\n",
        "        # split x into x1, x2\n",
        "        x1, x2 = x.chunk(2, dim=-1) # b, s, d/2\n",
        "        assert x1.size() == x2.size()\n",
        "        # rotate x1, x2\n",
        "        x1 = x1 * cos - x2 * sin # b, s, d/2\n",
        "        x2 = x1 * sin + x2 * cos # b, s, d/2\n",
        "        # concatenate\n",
        "        return torch.cat([x1, x2], dim=-1) # b, s, d\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.size(2) == self.max_seq:\n",
        "            con, sin = self.cos, self.sin\n",
        "        else:\n",
        "            cos, sin = self._get_cos_sin(seq_len=x.size(2), device=x.device)\n",
        "        return self._apply_rope(x, con, sin)\n"
      ],
      "metadata": {
        "id": "CdcjhgTl_twG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSNorm\n",
        "- LayerNorm: (x-μ)/σ. Shape of μ [batch_size, seq_length, 1].\n",
        "- weighted parameters: α * (x-μ)/σ + β. σ can be zero, catestrophic cancellation, lead to numeric instability.\n",
        "- RMSNorm: x/RMS(x): γ * x/RMS(x). Easy computing and avoid precision loss; also better gradient flow, each one is independent not relying on μ.\n",
        "- LayerNorm(x) = LayerNorm(x + c1). absolute position information is not captured. position [1,2,3] will have the same as position [10,20,30].\n",
        "- μ is not your friends empirically vulnerable to outliers.\n",
        "- BatchNorm: (x-μ)/σ. Shape of μ [1, 1, d_model]. Not good for LLM b/c batch variability can be large and different between training and testing. Even tricky with flexible input seq_len\n"
      ],
      "metadata": {
        "id": "-xRKFlfaE3hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        return self.gamma * x / (torch.norm(x, dim=-1, keepdim=True) + self.eps)\n",
        "\n",
        "# Pytorch has RMSNorm nn.RMSNorm(x)"
      ],
      "metadata": {
        "id": "O7xxsGoQIfsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SwiGLU activation\n",
        "- Similar to ReLU but with learned threshold (not like zero in ReLU)\n",
        "- Better gradient flow (allow non-zero gradient for negatives; avoid dieing ReLU or die neurons)\n",
        "- dead neurons: relu(x) = 0 receive 0 gradient, and stay 0 forever.\n",
        "- 2010s thinking: \"Dead neurons are bad, but sparsity is good\"\n",
        "- 2020s thinking: \"We want sparsity but not permenant dead\""
      ],
      "metadata": {
        "id": "UdsCiNGsdtMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLUFFN(nn.Module):\n",
        "    # SiLU(xW) * (xV)\n",
        "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
        "        super(SwiGLUFFN, self).__init__()\n",
        "        if d_ff is None:\n",
        "          d_ff = int(d_model * 8/3)\n",
        "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.up_proj= nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = F.silu(self.gate_proj(x))\n",
        "        up = self.up_proj(x)\n",
        "        hidden = gate * up\n",
        "        down = self.down_proj(self.dropout(hidden))\n",
        "        return down\n"
      ],
      "metadata": {
        "id": "hq7X_qsThYv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Query Attention (MQA)\n",
        "- shared k,v matrix across all headers\n",
        "- much faster, lower performance reduce\n"
      ],
      "metadata": {
        "id": "T3syxUjRxOuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, max_seq, rope=True):\n",
        "        super(MultiQueryAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, self.head_dim) # only head_dim, shared across headers\n",
        "        self.wv = nn.Linear(d_model, self.head_dim) # only head_dim, shared across headers\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(self.head_dim, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.wq(query) # b,s,d*n\n",
        "        key = self.wk(key) # b,s,d\n",
        "        value = self.wv(value) # b,s,d\n",
        "        # split\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.unsqueeze(1).expand(-1, query.size(1), -1, -1)\n",
        "        value = value.unsqueeze(1).expand(-1, query.size(1), -1, -1)\n",
        "        # optional\n",
        "        # RoPE for positional encoding\n",
        "        if self.rotary_embedding is not None:\n",
        "            query = self.rotary_embedding(query)\n",
        "            key = self.rotary_embedding(key)\n",
        "        # attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # mask\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        else:\n",
        "            scores = scores.masked_fill(torch.tril(torch.ones_like(scores, device=query.device)) == 0, -1e9)\n",
        "        # softmax\n",
        "        attention = F.softmax(scores, dim=-1) # b, n, s, s\n",
        "        # output\n",
        "        output = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # linear projection\n",
        "        return self.output_linear(output)"
      ],
      "metadata": {
        "id": "e_6HlWUmxcFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouped Query Attention\n",
        "- Compromise between MQA and MHA.\n",
        "- Group some headers to share K, V"
      ],
      "metadata": {
        "id": "RB4W_RtJzOrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, kv_groups, max_seq, rope=True):\n",
        "        super(GroupedQueryAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.kv_groups = kv_groups\n",
        "        self.heads_per_group = heads // kv_groups\n",
        "        assert self.heads_per_group * kv_groups == heads, \"heads must be divisible by kv_groups\"\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.group_dim = self.head_dim * self.kv_groups\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, self.group_dim)\n",
        "        self.wv = nn.Linear(d_model, self.group_dim)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(self.head_dim, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.wq(query) # b,s,d*n\n",
        "        key = self.wk(key) # b,s,d*m\n",
        "        value = self.wv(value) # b,s,d*m\n",
        "        # split\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.view(batch_size, -1, self.kv_groups, self.head_dim).transpose(1, 2).repeat_interleave(self.heads_per_group, dim=1)\n",
        "        value = value.view(batch_size, -1, self.kv_groups, self.head_dim).transpose(1, 2).repeat_interleave(self.heads_per_group, dim=1)\n",
        "        # rope\n",
        "        if self.rotary_embedding is not None:\n",
        "            query = self.rotary_embedding(query)\n",
        "            key = self.rotary_embedding(key)\n",
        "        # score\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        # mask\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        else:\n",
        "            scores = scores.masked_fill(torch.tril(torch.ones_like(scores, device=query.device)) == 0, -1e9)\n",
        "        # softmax\n",
        "        attention = F.softmax(scores, dim=-1) # b, n, s, s\n",
        "        # output\n",
        "        output = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # linear projection\n",
        "        return self.output_linear(output)\n"
      ],
      "metadata": {
        "id": "EMSXDZaXzifY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FlashAttention\n",
        "- The key concept of FlashAttention is there is a memory movement cost (which is the bottleneck) between  fast RAM or L1/L2 Caches (SRAM, 100TB/s) and global RAM in CUDA GPU (HBM/DRAM, 2TB/s). This is similar to I/O cost.\n",
        "- Everytime, pytorch line (or more accurately, a cuda kernel) will materialize (similar to writing out to disk) the variable in the global RAM.\n",
        "- However, we don't have to do that, if we calculate everything in HRAM, and store the calculated variables in the HRAM for later kernel operation\n",
        "- That will require us to break down the large matrix calculation into blocked matrix calculation (b/c the HRAM is smaller than global RAM)\n",
        "- But you can't really implement this by change the pytorch code b/c pytorch will still materialize the blocked matrix calculation results (determined by its kernel function).\n",
        "- The easiest you can do is to use `scaled_dot_product_attention`, which do blocked kernel calculation under the hood.\n",
        "- You can also use Triton, which is a python way to implement cuda kernels. see [this example](https://arunjitha.medium.com/simplifying-cuda-kernels-with-triton-a-pythonic-approach-to-gpu-programming-79bb7121e974)"
      ],
      "metadata": {
        "id": "yZ5_o2AD4oyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "\n",
        "class FlashAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, max_seq, rope=True):\n",
        "        super(FlashAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.head_dim = d_model // heads\n",
        "        assert self.head_dim * heads == d_model, \"d_model must be divisible by heads\"\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        if rope:\n",
        "            self.rotary_embedding = RotaryPositionalEmbedding(self.head_dim, max_seq)\n",
        "        else:\n",
        "            self.rotary_embedding = None\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # b, n, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.query_linear(query)\n",
        "        key = self.key_linear(key)\n",
        "        value = self.value_linear(value)\n",
        "        # split\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "        # optional\n",
        "        # RoPE for positional encoding\n",
        "        if self.rotary_embedding is not None:\n",
        "            query = self.rotary_embedding(query)\n",
        "            key = self.rotary_embedding(key)\n",
        "        # attention with flash_attention implemented under the hood\n",
        "        with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
        "            scores = scaled_dot_product_attention(query, key, value, attn_mask=mask, enable_gqa=False)\n",
        "        # output\n",
        "        return self.output_linear(scores)\n"
      ],
      "metadata": {
        "id": "cRCydqNV6Cjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention with Sliding window and Sparse Attention\n",
        "- mainly used in long-context scenario\n",
        "- sparse attention usually include global attention + local attention\n",
        "- global attention can be random attention; first_n token attention and stride-based attention;\n",
        "- the implementation is to create various mask where mask[i,j] = 1 means i will attend to j.\n",
        "- different attention can be combined as hierachical ways to form a bigbird mask\n",
        "- there are other attention ways like hash bucket based or other row-wise and column-wise based. But the overall idea is similar."
      ],
      "metadata": {
        "id": "8A_t039n19sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_local_masks(seq_len, local_window_size):\n",
        "    masks = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "    masks.fill_diagonal_(1)\n",
        "    for i in range(seq_len):\n",
        "        # gpt always attend to tokens before i\n",
        "        start = max(0, i - local_window_size)\n",
        "        end = i\n",
        "        masks[i, start:end] = True\n",
        "    return masks.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "def create_global_first_n_masks(seq_len, first_n):\n",
        "    masks = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "    masks.fill_diagonal_(1)\n",
        "    for i in range(seq_len):\n",
        "        start = 0\n",
        "        end = min(first_n, i)\n",
        "        masks[i, start:end] = True\n",
        "    return masks.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "def create_global_stride_masks(seq_len, stride):\n",
        "    masks = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "    masks.fill_diagonal_(1)\n",
        "    for i in range(seq_len):\n",
        "        start = 0\n",
        "        end = i\n",
        "        masks[i, start:end:stride] = True\n",
        "    return masks.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "def create_global_random_masks(seq_len, prob=0.1, device='cpu', seed=42):\n",
        "    # fix seed to ensure training and generating are same\n",
        "    generator = torch.Generator(device=device)\n",
        "    generator.manual_seed(seed)\n",
        "    masks = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "    masks.fill_diagonal_(1)\n",
        "    for i in range(1, seq_len):\n",
        "        random_vals = torch.rand(i, generator=generator, device=device)\n",
        "        masks[i, :i] = random_vals < prob\n",
        "    return masks.unsqueeze(0).unsqueeze(0)"
      ],
      "metadata": {
        "id": "MI69M3Oz79bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight Tieing\n",
        "- The embedding layer and last projection layer share parameters.\n",
        "- Intuitively it make sense, it also saves the paremeters (d_model * vocabulary_size)\n",
        "- Empirically, Llama implemented in that way.\n",
        "- Two ways to implement, in GPT style, same parameter will receive gradient from two paths embedding and final fc layer; In llama, simply multiple embedding weight to get logits, no gradient flow in the last layer."
      ],
      "metadata": {
        "id": "-foJxtsXBbgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTwithWeightTier(GPT2):\n",
        "    def __init__(self, vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout=0.1, rope=False, weight_tie=False):\n",
        "        super(GPTwithWeightTier, self).__init__(vocab_size, d_model, max_seq, n_layers, heads, d_ff, dropout, rope)\n",
        "        if weight_tie:\n",
        "            assert self.embedding is not None, \"embedding layer must be defined\"\n",
        "            self.fc = nn.linear(d_model, vocab_size, bias=False) # it is a function\n",
        "            self.fc.weight = self.embedding.weight\n",
        "    def forward(self, x, mask=None):\n",
        "        return super(GPTwithWeightTier, self).forward(x, mask)"
      ],
      "metadata": {
        "id": "13BXojOzBvyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixture of Experts (MoE)\n",
        "- replace the FFN after attention\n",
        "- Original TransformerBlock: LayerNorm -> Attention -> Residue -> LayerNorm -> FFN -> NextBlock\n",
        "- MoEBlock: LayerNorm -> Attention -> Residue -> Layernomr -> MoE -> NextBlock\n",
        "- The idea is instead of using only one FFN, we want to use weighted sum of num_experts number of FFNs (could be smaller d_ff). But we don't want to calculate num_experts for each token to increase the computational burden by num_experts times. Instead, we used a gated mechanism to only activate topk based on a gate_activation function.\n"
      ],
      "metadata": {
        "id": "sYeBzccQLTLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleExpertFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(SingleExpertFFN, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class GatedFFN(nn.Module):\n",
        "    def __init__(self, d_model, top_k, num_experts):\n",
        "        super(GatedFFN, self).__init__()\n",
        "        self.gate_proj = nn.Linear(d_model, num_experts, bias=False)\n",
        "        self.top_k = top_k\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def _compute_load_balance_loss(self, gate_scores, top_k_indices):\n",
        "        gate_prob = F.softmax(gate_scores, dim=-1) # b, s, e\n",
        "        mean_gate_prob = gate_prob.mean(dim=[0,1]) # e\n",
        "        tokens_per_experts = torch.zeros(self.num_experts, device=gate_scores.device) # e\n",
        "        for i in range(self.num_experts):\n",
        "            tokens_per_experts[i] = (top_k_indices == i).float().sum()\n",
        "        # normalize\n",
        "        total_tokens = gate_scores.size(0) * gate_scores.size(1) * self.top_k # b*s*k\n",
        "        token_usage_gate_prob = tokens_per_experts / total_tokens # e, sum(e) = 1\n",
        "        # compute loss - penalize deviations from uniform expert usage.\n",
        "        # similar to entropy but considered both continues prob (mean_gate_prob) and cut-off prob (token_usage)\n",
        "        return torch.sum(token_usage_gate_prob * mean_gate_prob) * self.num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate_scores = self.gate_proj(x) # b, s, e\n",
        "        top_k_scores, top_k_indices = torch.topk(gate_scores, self.top_k, dim=-1) # b, s, k\n",
        "        top_k_gate_weight = F.softmax(top_k_scores, dim=-1) # b, s, k\n",
        "\n",
        "        if self.training:\n",
        "            load_balance_loss = self._compute_load_balance_loss(gate_scores, top_k_indices)\n",
        "\n",
        "        else:\n",
        "            load_balance_loss = torch.tensor(0.0, device=x.device)\n",
        "\n",
        "        return top_k_gate_weight, top_k_indices, load_balance_loss\n",
        "\n",
        "class MoEFFN(nn.Module):\n",
        "    def __init__(self, d_model, top_k, d_ff, num_experts, dropout=0.1):\n",
        "        super(MoEFFN, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.expert_ffns = nn.ModuleList([SingleExpertFFN(d_model, d_ff, dropout) for _ in range(num_experts)])\n",
        "        self.gate_activation = GatedFFN(d_model, top_k, num_experts)\n",
        "        self.top_k = top_k\n",
        "    def forward(self, x):\n",
        "        top_k_gate_weight, top_k_indices, load_balance_loss = self.gate_activation(x) # b,s,k\n",
        "        flattened_x = x.view(-1, x.size(-1)) # b*s, d -> b*s tokens, each token has own top_k gate and prob\n",
        "        flattened_top_k_gate_weight = top_k_gate_weight.view(-1, top_k_gate_weight.size(-1)) # b*s, k\n",
        "        flattened_top_k_indices = top_k_indices.view(-1, top_k_indices.size(-1)) # b*s, k\n",
        "        weighted_expert_outputs = torch.zeros_like(flattened_x) # b*s, d\n",
        "        for j in range(self.top_k):\n",
        "          # get current expert idx\n",
        "          top_k_expert_idx = flattened_top_k_indices[:, j] # b*s\n",
        "          top_k_expert_weight = flattened_top_k_gate_weight[:, j] # b*s\n",
        "          # process current j position for each token\n",
        "          for i in range(self.num_experts):\n",
        "              expert_mask = (top_k_expert_idx == i) # b*s\n",
        "              if expert_mask.any():\n",
        "                # only process expert with at least one tokens\n",
        "                expert_input = flattened_x[expert_mask] # ?, d\n",
        "                expert_output = self.expert_ffns[i](expert_input) # ?, d\n",
        "                expert_weight = top_k_expert_weight[expert_mask].unsqueeze(-1) # ?, 1\n",
        "                weighted_expert_outputs[expert_mask] += expert_weight * expert_output # ?, 1\n",
        "\n",
        "        return weighted_expert_outputs.view(x.size(0), x.size(1), -1),  load_balance_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "lIT6kUczM9LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAg22vZyu0jB"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi9LfGm0mgrz"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.seq = self.tokenizer.encode(data)\n",
        "        self.seq_num = len(self.seq) - max_length - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seq_num\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = self.seq[idx:idx+self.max_length]\n",
        "        target = self.seq[idx+1:idx+self.max_length+1]\n",
        "        return torch.tensor(input, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhwqEfFfu6Y3"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0yPy2WTniTC"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "class Trainer:\n",
        "    def __init__(self, model, device, learning_rate = 1e-3, weight_decay = 0.01, warmup_steps = 1000, max_steps = 10000, gradient_accumulation_steps = 1, grad_clip = 1.0, save_dir = \"./checkpoints\"):\n",
        "        self.model = model\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.scheduler = self._get_scheduler(warmup_steps, max_steps)\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.grad_clip = grad_clip\n",
        "        self.save_dir = save_dir\n",
        "        self.global_step = 0\n",
        "        self.best_loss = float('inf')\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    def _get_scheduler(self, warmup_steps, max_steps):\n",
        "        def lr_lambda(current_step):\n",
        "            if current_step < warmup_steps:\n",
        "                return float(current_step) / float(max(1, warmup_steps))\n",
        "            # Cosine decay after warmup\n",
        "            progress = float(current_step - warmup_steps) / float(max(1, max_steps - warmup_steps))\n",
        "            return max(0.0, 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(progress))))\n",
        "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "\n",
        "    def train(self, train_dataloader, validate_dateloader, epochs):\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            total_loss = 0\n",
        "            pb = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                loss = loss / self.gradient_accumulation_steps if self.gradient_accumulation_steps > 1 else loss\n",
        "                loss.backward()\n",
        "                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
        "                    self.optimizer.step()\n",
        "                    self.scheduler.step()\n",
        "                    self.global_step += 1\n",
        "                # update pb for loss\n",
        "                total_loss += loss.item() * self.gradient_accumulation_steps\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / (batch_idx + 1):4f}\")\n",
        "            # call validate\n",
        "            self.validate(validate_dateloader)\n",
        "        print(f\"Training finished\")\n",
        "        return 1\n",
        "\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        pb = tqdm(dataloader, desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                total_loss += loss.item()\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / (batch_idx + 1):4f}\")\n",
        "\n",
        "        if (total_loss / (batch_idx + 1)) < self.best_loss:\n",
        "            self.best_loss = (total_loss / (batch_idx + 1))\n",
        "            self.save_checkpoint()\n",
        "\n",
        "        return 1\n",
        "\n",
        "    def save_checkpoint(self, filename='best_model.pt'):\n",
        "        checkpoint_path = os.path.join(self.save_dir, filename)\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'global_step': self.global_step,\n",
        "            'best_loss': self.best_loss\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    def load_checkpoint(self, filename='best_model.pt'):\n",
        "        checkpoint_path = os.path.join(self.save_dir, filename)\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        self.model_config = checkpoint['model_config']\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.global_step = checkpoint['global_step']\n",
        "        self.best_loss = checkpoint['best_loss']\n",
        "        print(f\"Checkpoint loaded from {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Parallel or DDP (Distributed Data Parallel)\n",
        "- across multiple nodes (x gpu per nodes)\n",
        "- each node contains same dataset replica, model and environment\n",
        "- node know each other through world size and master node\n",
        "- each iteration update same model (model wrapped using DDP), same parameter, but different sampled dataset (from a same dataset) across nodes\n",
        "- back propogation (loss) auto syncronized (all reduced) through torch.distributed\n",
        "- able to process larger batch size (128 in one node with 2 gpu -> 128 * 8 with 4 nodes with 2 gpus), improve training efficiency\n",
        "- only save one copy and print one loss b/c they are all syned across nodes (rank = 0)\n",
        "- support both gpu (GLOO) and cpu (NCCL) training\n",
        "- use either torchrun (production, better fault tolerance) or multiprocessing.spawn\n"
      ],
      "metadata": {
        "id": "H7I8PSimVcdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "\n",
        "class DDPTrainer:\n",
        "    def __init__(self, model, world_size,, learning_rate = 1e-3, weight_decay = 0.01, warmup_steps = 1000, max_steps = 10000, gradient_accumulation_steps = 1, grad_clip = 1.0, save_dir = \"./checkpoints\"):\n",
        "        self.model = model\n",
        "        self.world_size = world_size\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.grad_clip = grad_clip\n",
        "        self.save_dir = save_dir\n",
        "        self.global_step = 0\n",
        "        self.best_loss = float('inf')\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_setup(rank, world_size, backend=\"nccl\"):\n",
        "        # nccl for gpu; gloo for cpu\n",
        "        os.environ['MASTER_ADDR'] = 'localhost'\n",
        "        os.environ['MASTER_PORT'] = '12355'\n",
        "        torch.cuda.set_device(rank)\n",
        "        dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def _cleanup():\n",
        "        dist.destroy_process_group()\n",
        "\n",
        "    def _get_scheduler(self, warmup_steps, max_steps):\n",
        "        def lr_lambda(current_step):\n",
        "            if current_step < warmup_steps:\n",
        "                return float(current_step) / float(max(1, warmup_steps))\n",
        "            # Cosine decay after warmup\n",
        "            progress = float(current_step - warmup_steps) / float(max(1, max_steps - warmup_steps))\n",
        "            return max(0.0, 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(progress))))\n",
        "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "\n",
        "    def train_master(self, train_dataset, val_dataloader, epochs, batch_size):\n",
        "        # master to spawn multiprocess\n",
        "        mp.spawn(self.train_worker, args=(train_dataset, val_dataloader, epochs, batch_size), nprocs=self.world_size, join=True)\n",
        "        return 1\n",
        "\n",
        "\n",
        "    def train_worker(self, rank, train_dataset, val_dataloader, epochs, batch_size):\n",
        "\n",
        "        # init progress group\n",
        "        self._init_setup(rank, self.world_size)\n",
        "\n",
        "        self.device = torch.device(f'cuda:{rank}')\n",
        "\n",
        "        # init model for this worker\n",
        "        self.model = self.model.to(self.device)\n",
        "        model = DDP(self.model, device_ids=[rank], output_device=rank)\n",
        "        model.train()\n",
        "\n",
        "        # Initialize optimizer, scheduler, and criterion for this worker\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        self.scheduler = self._get_scheduler(self.optimizer, self.warmup_steps, self.max_steps)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        #  load dataset - the entire data copy is needed in each node\n",
        "        train_dataset_sampler = DistributedSampler(self.train_dataset, num_replicas=self.world_size, rank=rank)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_dataset_sampler, pin_memory=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Set epoch for sampler to ensure different shuffling each epoch\n",
        "            train_sampler.set_epoch(epoch)\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            if rank == 0:\n",
        "                pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "            else:\n",
        "                pbar = train_dataloader\n",
        "            for batch_idx, (data, target) in enumerate(pbar):\n",
        "                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                loss = loss / self.gradient_accumulation_steps if self.gradient_accumulation_steps > 1 else loss\n",
        "                loss.backward()\n",
        "                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
        "                    self.optimizer.step()\n",
        "                    self.scheduler.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    self.global_step += 1\n",
        "                # update pb for loss\n",
        "                total_loss += loss.item() * self.gradient_accumulation_steps\n",
        "                num_batches += 1\n",
        "                # Update progress bar\n",
        "                if batch_idx % 100 == 0 and rank == 0: # only print in node 0.\n",
        "                    average_loss = total_loss / num_batches\n",
        "                    pbar.set_postfix(loss=f\"{average_loss:.4f}\", step=self.global_step)\n",
        "\n",
        "            # call validate\n",
        "            if rank == 0:\n",
        "              self.validate(model, validate_dateloader)\n",
        "\n",
        "        self._cleanup()\n",
        "        return 1\n",
        "\n",
        "    def validate(self, model, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        number_batches = 0\n",
        "\n",
        "        pb = tqdm(dataloader, desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / num_batches:4f}\")\n",
        "\n",
        "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "\n",
        "        if avg_loss < self.best_loss:\n",
        "            self.best_loss = avg_loss\n",
        "            self.save_checkpoint(model, loss)\n",
        "\n",
        "        return 1\n",
        "\n",
        "    def save_checkpoint(self, model, loss):\n",
        "        checkpoint_path = os.path.join(self.save_dir, 'best_model.pt')\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'global_step': self.global_step,\n",
        "            'best_loss': self.best_loss\n",
        "            'loss': loss\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    world_size = 4\n",
        "    trainer = DDPTrainer(model, world_size)\n",
        "    train_dataset = TextDataset(train_data[1:10000], tokenizer, model_config['max_seq'])\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
        "    epochs = 100\n",
        "    trainer.train_master(train_dataset, val_dataloader, epochs)\n"
      ],
      "metadata": {
        "id": "Og_HBFiQW_Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor or Model Parallel\n",
        "- DDP is faster but the bottleneck in modern LLM is GPU RAM.\n",
        "- What if the model and its parameters can not fit inside one GPU?\n",
        "- We have to parallel and share models parameters across GPUs.\n",
        "- Column parallel for a linear layer [int, out] -> [int, out/number_of_gpus]\n",
        "- Row parallel for a linear layer [int, out] -> [int/number_of_gpus, out]\n",
        "- use `dist.all_reduce`, `dist_all_gather` to communicate across devices"
      ],
      "metadata": {
        "id": "aN4u5LQYl97N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "\n",
        "class ColumnParallelLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank, world_size, gather_output=True):\n",
        "        super(ShardedLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.world_size = world_size\n",
        "        self.rank = rank\n",
        "        self.local_out_features = out_features // world_size\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, self.local_out_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.local_out_features))\n",
        "        self.gather_output = gather_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        local_output = F.linear(x, self.weight.t(), self.bias) # b, s, d // ws\n",
        "        if not self.gather_output:\n",
        "            return local_output\n",
        "        output_list = [torch.zeros_like(local_output) for _ in range(self.world_size)]\n",
        "        dist.all_gather(output_list, local_output) # b, s, d // ws\n",
        "        # cat\n",
        "        output = torch.cat(output_list, dim=-1)\n",
        "        return output\n",
        "\n",
        "def train_workder(rank, world_size, in_features, out_features):\n",
        "    # setup env. etc.\n",
        "\n",
        "    device = torch.device(f'cuda:{rank}')\n",
        "    model = ShardedLinear(in_features, out_features, rank, world_size).to(device)\n",
        "    model.train()\n",
        "\n",
        "    x = torch.randn(10, in_features).to(device)\n",
        "    y = model(x)\n",
        "    # loss update\n",
        "\n",
        "    # clean up\n",
        "\n",
        "\n",
        "\n",
        "# training example\n",
        "# init dist setup ,etc.\n",
        "in_features = 1024\n",
        "out_features = 2048\n",
        "rank = 0\n",
        "world_size = 4\n",
        "mp.spawn(train_workder, args=(world_size,in_features, out_features), nprocs=world_size, join=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "-Qn4b7P1ms0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Parallel\n",
        "- tensor parallel still have to gather the output, which cost lots of memory\n",
        "- for deeper networks, we can create a pipe to with different stage assigned to different gpus\n",
        "- during training, the initial x should be in first gpu, and target should be in last gpu\n",
        "- pipe update will handle the update"
      ],
      "metadata": {
        "id": "XRLRGCzAZSfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "from torch.distributed.pipeline.sync import pipe\n",
        "\n",
        "\n",
        "class TinyBlocks(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def build_a_pipeline(world_size, number_layers, in_features, hidden_features):\n",
        "    assert number_layers % world_size == 0, \"World size must be divisible by number of layers\"\n",
        "    layers_per_gpu = number_layers // world_size\n",
        "    layers = []\n",
        "\n",
        "    for rank in range(number_layers):\n",
        "        layers.append(TinyBlocks(in_features, hidden_features))\n",
        "\n",
        "    # create pipeline\n",
        "    model = nn.Sequential(*layers)\n",
        "    devices = [torch.device(f'cuda:{i}') for i in range(world_size)]\n",
        "    # use chunk to reduce gpu idle time, process another batch while waiting for forward pass on other gpus\n",
        "    # pros: faster; cons: high GPU RAM, two batch activation are stored\n",
        "    pipeline = pipe(model, balance = [layers_per_gpu] * world_size, devices = devices, chunk = 8)\n",
        "    return pipeline\n",
        "\n",
        "in_features = 1024\n",
        "hidden_features = 2048\n",
        "world_size = 8\n",
        "number_layers = 4\n",
        "# init setup\n",
        "pipeline = build_a_pipeline(world_size, number_layers, in_features, hidden_features)\n",
        "\n",
        "first = torch.device(f'cuda:0')\n",
        "last = torch.device(f'cuda:{world_size - 1}')\n",
        "\n",
        "optim = torch.optim.Adam(pipeline.parameters())\n",
        "for _ in range(10):\n",
        "  optim.zero_grad(set_to_none=True)\n",
        "  x = torch.randn(10, in_features).to(first)\n",
        "  target = torch.randn(10, in_features).to(last)\n",
        "  pipeline.to(first)\n",
        "  pipeline.train()\n",
        "  y = pipeline(x)\n",
        "  # loss update\n",
        "  loss = F.mse_loss(y, target)\n",
        "  loss.backward()\n",
        "  # update optim\n",
        "  optim.step()\n",
        "# clean up\n",
        "\n"
      ],
      "metadata": {
        "id": "hmADVe1bZ0Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expert Parallel\n",
        "- Distribute expert networks into different GPUs\n",
        "- Route tokens to different GPUs for expert embedding calculation\n",
        "- all_to_all"
      ],
      "metadata": {
        "id": "F8FTXwBA3-09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoEFFNParallel(nn.Module):\n",
        "    def __init__(self, world_size, rank, d_model, top_k, d_ff, num_experts, dropout=0.1):\n",
        "      super(MoEFFNParallel, self).__init__()\n",
        "      assert num_experts % world_size == 0, \"Number of experts must be divisible by world size\"\n",
        "\n",
        "      # Store distributed training configuration\n",
        "      self.rank = rank # Current GPU rank\n",
        "      self.world_size = world_size # Total number of GPUs\n",
        "      self.expert_per_rank = num_experts // world_size # Number of experts per GPU\n",
        "      self.num_experts = num_experts # Total number of experts\n",
        "      # self.expert_ffns = nn.ModuleList([SingleExpertFFN(d_model, d_ff, dropout) for _ in range(num_experts)])\n",
        "\n",
        "      # Get local experts for this rank only\n",
        "      self.local_expert_ffns = nn.ModuleList([SingleExpertFFN(d_model, d_ff, dropout) for _ in range(self.expert_per_rank)])\n",
        "      self.gate_activation = GatedFFN(d_model, top_k, num_experts) # replicated in all ranks\n",
        "      self.top_k = top_k # Number of top experts to use per token\n",
        "\n",
        "      # Create expert parallelism process group for communication\n",
        "      self.ep_group = None # Should be initialized externally dist.new_group([rank for rank in range(world_size)])\n",
        "\n",
        "    def _which_rank(self, top_k_indices):\n",
        "      # Determine which GPU rank owns each expert\n",
        "      return top_k_indices // (self.expert_per_rank)\n",
        "\n",
        "    def _which_local_expert_idx(self, top_k_indices):\n",
        "      # Determin local experts idx in self.local_expert_ffns within a rank\n",
        "      return top_k_indices % (self.expert_per_rank)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      B, S, H = x.size() # Each GPU rank start with same tokens\n",
        "      K = self.top_k\n",
        "\n",
        "      # Gate activation is same as the regular MoE. self.gate_activation is stored in each rank\n",
        "      # All GPUs compute the same gating decision\n",
        "      top_k_gate_weight, top_k_indices, load_balance_loss = self.gate_activation(x) # b,s,k\n",
        "      assert top_k_gate_weight.ndim == 3, \"top_k_gate_weight should have 3 dimensions\"\n",
        "      assert top_k_indices.ndim == 3, \"top_k_indices should have 3 dimensions\"\n",
        "\n",
        "      # (1) flat to token-expert pairs B*S*K\n",
        "      flattened_x = x.repeat_interleave(k, dim=1).reshape(-1, H). # (b*s*k,H)\n",
        "      flattened_top_k_gate_weight = top_k_gate_weight.reshape(-1) # b*s*k\n",
        "      flattened_top_k_indices = top_k_indices.reshape(-1) # b*s*k\n",
        "\n",
        "      # track source token indices for tracking where each token came from\n",
        "      src = torch.arange(B*S, device=x.device).repeat_interleave(k) # b*s*k [1,2,3] -> [1,1,2,2,3,3]\n",
        "\n",
        "      # (2) determine routing, figure out the top_k ranks and local expert idx\n",
        "      flattened_top_k_ranks = self._which_rank(flattened_top_k_indices) # b*s*k [1,2,3,2,3,4]\n",
        "      flattened_top_k_local_expert_idx = self._which_local_expert_idx(flattened_top_k_indices) # b*s*k [0,0,0,0,0,0]\n",
        "\n",
        "      # (3) Pack tokens destined for each rank\n",
        "      # Each GPU organizes: \"What do I need to send to each other GPU?\"\n",
        "      # Of course, it can send to itself, if its expert is in the same rank\n",
        "      per_dest_tokens = [] # list of [?,H] tokens going to each rank if rank=2; [[d],[d]]\n",
        "      per_dest_local_expert_idx = [] # list of [?] local expert indices for each rank [0,0]\n",
        "      per_dest_gate_weight = [] # list of [?] gate weight for each rank [w, w]\n",
        "      per_dest_src = [] # list of [?] source token indices for each rank [1,2]\n",
        "\n",
        "\n",
        "      for dest in range(self.world_size):\n",
        "\n",
        "        # find all assignment going to the GPU rank\n",
        "        mask = (flattened_top_k_ranks == dest) # [b*s*k] row-wise mask [0,1,0,1,0,1]\n",
        "        if not torch.any(mask): # [0,0,0,0,0,0]\n",
        "          # no tokens going to the rank - create empty tensors\n",
        "          per_dest_tokens.append(torch.empty(0, H, device=x.device, dtype=x.dtype)) #[0,H]\n",
        "          per_dest_local_expert_idx.append(torch.empty(0, device=x.device, dtype=torch.long))\n",
        "          per_dest_gate_weight.append(torch.empty(0, device=x.device, dtype=x.dtype))\n",
        "          per_dest_src.append(torch.empty(0, device=x.device, dtype=torch.long))\n",
        "          continue\n",
        "        # append to list\n",
        "        # This GPU rank prepares to send:\n",
        "        # To GPU 0: Token A (for expert 1), Token C (for expert 0)\n",
        "        # To GPU 1: Token B (for expert 2), Token C (for expert 3)\n",
        "        # To GPU 2: Token A (for expert 5)\n",
        "        # To GPU 3: Token B (for expert 6)\n",
        "        per_dest_tokens.append(flattened_x[mask]) # (?, H, r)\n",
        "        per_dest_local_expert_idx.append(flattened_top_k_local_expert_idx[mask]) # (?, r)\n",
        "        per_dest_gate_weight.append(flattened_top_k_gate_weight[mask]) # (?, r)\n",
        "        per_dest_src.append(src[mask]) # hold the single token (B*S) id -> used for aggregation later. # (?, r)\n",
        "\n",
        "\n",
        "      # 4) Exchange counts, then variable-size all_to_all for each payload\n",
        "      # some GPUs might receive more tokens than others\n",
        "      # GPU 0 receives:\n",
        "      # From GPU 0: Token A (for local expert 1), Token C (for local expert 0)\n",
        "      # From GPU 1: Token A (for local expert 1), Token C (for local expert 0)\n",
        "      # From GPU 2: Token A (for local expert 1), Token C (for local expert 0)\n",
        "      # From GPU 3: Token A (for local expert 1), Token C (for local expert 0)\n",
        "      send_counts = torch.tensor([t.size(0) for t in per_dest_tokens], device=x.device, dtype=torch.int64) # r, send to GPU:0, GPU:1, GPU:2, GPU:3 - [104, 123, 12, 0]\n",
        "      recv_counts = torch.empty_like(send_counts) # r, initialize the receive buffer for each GPU [?,?,?,?]\n",
        "\n",
        "      # NCCL Implements ring all-to-all rather than N*(N-1) sequential exchange.\n",
        "      # each round send to x steps away neighbor in the circle\n",
        "      # N=3, round 1: gpu0 send to gpu1, gpu1 send to gpu2, gpu2 send to gpu0,\n",
        "      # round 2:  gpu0 send to gpu2, gpu1 send to gpu0, gpu2 send to gpu1\n",
        "      # in total N-1 round needed.\n",
        "      dist.all_to_all_single(recv_counts, send_counts, group=self.ep_group) # All-to-all communication to exchange counts\n",
        "      # After this, for this rank recv_counts [12,123,11,0] received from GPU0, 1, 2,3\n",
        "      # !!!Important in the MoE setting, send_counts are different for all ranks\n",
        "      # because each GPU process different data batch through DDP\n",
        "      total_send = int(send_counts.sum().item()) # Total tokens we'are sending for this GPU rank.\n",
        "      assert total_send == B*S*K, \"total_send should be equal to B*S*K\"\n",
        "      total_recv = int(recv_counts.sum().item())   # Total tokens we're receiving for this GPU rank - allocate the right-sized receive buffers\n",
        "\n",
        "\n",
        "      # Help function Flatten lists - Prepare data for all to all exchange\n",
        "      def cat1D(lst):\n",
        "        return torch.cat(lst, dim=0) if lst and any(t.numel() > 0 for t in lst) else torch.empty(0, device=x.device)\n",
        "\n",
        "\n",
        "      send_tokens_1d = cat1D(per_dest_tokens).reshape(-1) # [B*S*K*H]\n",
        "      send_dest_local_expert_idx = cat1D(per_dest_local_expert_idx) # [B*S*K]\n",
        "      send_gate_weight = cat1D(per_dest_w) # [B*S*K]\n",
        "      send_src = cat1D(per_dest_src) # [B*S*K]\n",
        "\n",
        "\n",
        "      # Token vectors use element counts (Ni*H) splits\n",
        "      # [token1,token1,...(H),token1, ..., token_send_counts]\n",
        "      # For token vectors: each token has H elements, so multiply counts by H\n",
        "      send_splits_el = (send_counts * H).tolist()\n",
        "      recv_splits_el = (recv_counts * H).tolist()\n",
        "\n",
        "      # Prepare receive buffers\n",
        "      recv_tokens_1d = torch.empty(total_recv * H, device=x.device, dtype=x.dtype)\n",
        "      recv_dest_local_expert_idx = torch.empty(total_recv, device=x.device, dtype=torch.long)\n",
        "      recv_gate_weight = torch.empty(total_recv, device=x.device, dtype=x.dtype)\n",
        "      recv_src = torch.empty(total_recv, device=x.device, dtype=torch.long)\n",
        "\n",
        "      # perform All-to-All token, expert id, weight and src, exchange\n",
        "      dist.all_to_all_single(recv_tokens_1d, send_tokens_1d, recv_splits_el, send_splits_el, group=self.ep_group)\n",
        "      dist.all_to_all_single(recv_dest_local_expert_idx, send_dest_local_expert_idx, recv_counts.tolist(), send_counts.tolist(), group=self.ep_group)\n",
        "      dist.all_to_all_single(recv_gate_weight, send_gate_weight, recv_counts.tolist(), send_counts.tolist(), group=self.ep_group)\n",
        "      dist.all_to_all_single(recv_src, send_src, recv_counts.tolist(), send_counts.tolist(), group=self.ep_group)\n",
        "\n",
        "      # Reshape received tokens back to 2D\n",
        "      recv_tok = recv_tok_1d.view(total_recv, H) # [?, H]\n",
        "\n",
        "\n",
        "      # (5) Local compute per expert, then weight by combine and place back\n",
        "      y_local = torch.empty_like(recv_tok) # initialize output buffer -> ?, H\n",
        "      if total_recv > 0:\n",
        "        # Process each local expert separately\n",
        "        for le in range(self.experts_per_rank):\n",
        "          # Find tokens assigned to this local expert\n",
        "          mask = (recv_dest_local_expert_idx == le)\n",
        "          if not torch.any(mask): # no tokens for this expert\n",
        "            continue\n",
        "          # Run expert forward pass\n",
        "          expert_input = recv_tok[mask]\n",
        "          expert_output = self.local_expert_ffns[le](expert_input)  # Fixed attribute name\n",
        "          # Apply gate weights and store results\n",
        "          weighted_output = expert_output * recv_gate_weight[mask].unsqueeze(-1)\n",
        "          y[mask] = weighted_output\n",
        "\n",
        "      # 6) Send results back to source ranks (reverse splits) e.g. received from GPU 0 will send back to GPU 0\n",
        "      send_back_tok_1d = y_local.reshape(-1)     # Flatten for communication ?*H\n",
        "      recv_back_tok_1d = torch.empty(total_send * H, device=x.device, dtype=x.dtype) # received buffer from other gpus  [B*S*K*H]\n",
        "      # Also send back source token-indices (in B*S*K) so we know where to add results\n",
        "      send_back_src = recv_src\n",
        "      recv_back_src = torch.empty(total_send, device=x.device, dtype=torch.long) # [B*S*K]\n",
        "      send_back_splits_el = recv_splits_el       # Reverse the split sizes\n",
        "      recv_back_splits_el = send_splits_el\n",
        "\n",
        "      dist.all_to_all_single(recv_back_tok_1d, send_back_tok_1d, recv_back_splits_el, send_back_splits_el, group=self.ep_group)\n",
        "      dist.all_to_all_single(recv_back_src, send_back_src, send_counts.tolist(), recv_counts.tolist(), group=self.ep_group)\n",
        "\n",
        "\n",
        "      # 7) calculate weighted (per expert) sum from token-expert pair to token\n",
        "      y_flat = torch.zeros(B * S, H, device=x.device, dtype=x.dtype) #[B*S, H] main B*S token sequence.\n",
        "      y_back_2d = y_back_1d.view(total_send, H) # [B*S*K, H]\n",
        "      y_flat.index_add_(0, recv_back_src, y_back_2d)  # weighted sum: first position, ranged by GPU now\n",
        "      return y_flat.view(B, S, H), load_balance_loss\n",
        "\n",
        "# Example usage and initialization\n",
        "def train_parallel_moe():\n",
        "  \"\"\"Example of how to set up the distributed MoE\"\"\"\n",
        "  # Initialize distributed training\n",
        "  dist.init_process_group(backend='nccl')\n",
        "  # Get rank and world size\n",
        "  rank = dist.get_rank()\n",
        "  world_size = dist.get_world_size()\n",
        "  torch.cuda.set_device(rank)\n",
        "\n",
        "  # Create expert parallelism group (all ranks participate)\n",
        "  ep_group = dist.new_group(list(range(world_size)))\n",
        "\n",
        "  # Create MoE layer\n",
        "  moe = MoEFFNParallel(\n",
        "      world_size=world_size,\n",
        "      rank=rank,\n",
        "      d_model=512,\n",
        "      top_k=2,\n",
        "      d_ff=2048,\n",
        "      num_experts=8,\n",
        "      dropout=0.1\n",
        "  ).cuda(rank)\n",
        "\n",
        "  # Set the process group\n",
        "  moe.ep_group = ep_group\n",
        "\n",
        "  # DDP shared layers, in full implementation, include emb, att, norm, etc.\n",
        "  moe.GatedFFN = DDP(moe.GatedFFN, device_ids=[rank])\n",
        "\n",
        "  # create opt for shared and not shared parameters\n",
        "  shared_parameters = list(moe.local_expert_ffns.parameters())\n",
        "  local_parameters = list(moe.GatedFFN.parameters())\n",
        "  shared_optimizer = torch.optim.Adam(shared_parameters + not_shared_parameters, lr=0.001)\n",
        "  local_optimizer = torch.optim.Adam(local_parameters, lr=0.001)\n",
        "\n",
        "  # training\n",
        "  epoch = 10\n",
        "  for _ in range(epoch):\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "      y = moe(data)\n",
        "      loss = F.mse_loss(y, target)\n",
        "      loss.backward()\n",
        "      shared_optimizer.step()\n",
        "      local_optimizer.step()\n",
        "      shared_optimizer.zero_grad()\n",
        "      local_optimizer.zero_grad()\n",
        "  return moe"
      ],
      "metadata": {
        "id": "uYSPe8UH4LQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ZeRO (Zero Redundancy Optimizer)\n",
        "- Optimizer stores parameters, gradient and states therefore cost lots of memory\n",
        "- For SGD, the state is just learning rate, so it is cheap. But in AdamW, there is m (first momentum) and v (second momentum) per parameter, so the state memory is 2*parameter - very expensive.\n",
        "- ZeRO is to shard state.\n",
        "- FSDP - shard state, gradient, and parameters. Reduces memory mostly but adds coordination during step().\n",
        "- DeepSpeed - shard state and gradient (middle ground)"
      ],
      "metadata": {
        "id": "8rvOkwdNz5mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distribute.optim import ZeroRedundancyOptimizer\n",
        "\n",
        "class ZeROTrainer(Trainer):\n",
        "    def __init__(self, model, device, learning_rate = 1e-3, weight_decay = 0.01, warmup_steps = 1000, max_steps = 10000, gradient_accumulation_steps = 1, grad_clip = 1.0):\n",
        "        super(ZeROTrainer, self).__init__(model, device, learning_rate, weight_decay, warmup_steps, max_steps, gradient_accumulation_steps, grad_clip)\n",
        "\n",
        "    # overwrite train_worker with ZeroRedundancyOptimizer\n",
        "    def train_worker(self, rank, train_dataset, val_dataloader, epochs, batch_size):\n",
        "      self.optimizer = ZeroRedundancyOptimizer(self.optimizer, self.model.parameters())\n",
        "\n"
      ],
      "metadata": {
        "id": "hpBRok7-1XFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixed Precision\n",
        "- using float16 instead of fp32 to save memory and accelerate operation\n",
        "- NVIDIA tensor core can handle matrix multiplication with fp16 much faster\n",
        "- add operation using fp32 to maintain numerical stability, multiplication using fp16 to make it faster\n",
        "- using torch autocaster context to make it stable\n",
        "- other format - fp32 (no tensor core), fp16, bf16, tf32, etc.\n",
        "- bf16 is the truncated version of fp32 (truncate fractional bits). cheap conversion cost. supported by TPU and A100/H100, RTX30/40."
      ],
      "metadata": {
        "id": "FLHEcQWFWgqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MixedPrecisionTrainer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.scaler = GradScaler()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters())\n",
        "        assert torch.cuda.is_available(), \"CUDA is not available\"\n",
        "        self.device = torch.device(\"cuda\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, train_dataloader, epochs):\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            total_loss = 0\n",
        "            pb = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "                if torch.cuda.is_bf16_supported():\n",
        "                    dtyple_cast = torch.bfloat16\n",
        "                else:\n",
        "                    dtype_cast = torch.float16\n",
        "\n",
        "                with autocast(dtype=dtype_cast): # cast to fp16 or bf16 for matmul operations\n",
        "                    output = self.model(data)\n",
        "                    loss = self.criterion(output, target)\n",
        "                self.scaler.scale(loss).backward() # scale the number by a constant to avoid fp16 underflow\n",
        "                self.scaler.step(self.optimizer) # unscale gradients before applying them\n",
        "                self.scaler.update() # adjust scale factor\n",
        "                self.optimizer.zero_grad()\n",
        "                total_loss += loss.item()\n",
        "                # update pb per 100\n",
        "                if batch_idx % 100 == 0:\n",
        "                    pb.set_postfix(loss=f\"{total_loss / (batch_idx + 1):4f}\")\n",
        "\n",
        "            # validate\n",
        "            self.validate(validate_dataloader)\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        pb = tqdm(dataloader, desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(pb):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                with autocast(): # ops (e.g.matmul) using FP16, ops (e.g. softmax) using FP32\n",
        "                    output = self.model(data)\n",
        "                    loss = self.criterion(output, target)\n",
        "                    total_loss += loss.item()"
      ],
      "metadata": {
        "id": "oqg6eRZRW7da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Checkpoint or Activation Recomputation\n",
        "- During forward pass, activation/output/x of each layers will be calculated and stored, this is needed to calculate gradient in the backward. This will need lots of GPU RAM.\n",
        "- Solution, we don't store all x. We only store some x in the GPU RAM, and calculate the other x (e.g. x = relu(x), only store the x passed to relu) again when it is needed in backward pass.\n",
        "- trade-off between RAM and computing time\n",
        "- set preserve_rng_state = True to allow same mask inside those discarded hidden layers"
      ],
      "metadata": {
        "id": "p8LKvk4VWDxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class BigNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, layer_num, use_gc = True):\n",
        "        super(BigNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.blocks = nn.ModuleList([SimpleNN(hidden_size, hidden_size, hidden_size) for _ in range(layer_num)])\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.use_gc = use_gc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        for block in self.blocks:\n",
        "            if self.use_gc:\n",
        "                # set preserve_rng_state = True if drop out in block\n",
        "                x = checkpoint(block, x, use_reentrant=False, preserve_rng_state=False) # don't store the hidden layer output in simpleNN\n",
        "            else:\n",
        "                x = block(x)\n",
        "        return x\n",
        "\n",
        "# A minimal training example\n",
        "def run_one_batch(use_gc):\n",
        "    device = \"cuda\"\n",
        "    model = Model(size=4096, depth=12, use_gc=use_gc).to(device)\n",
        "    data = torch.randn(32, 4096, device=device, requires_grad=True)\n",
        "    target = torch.randn(32, 4096, device=device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # check memory\n",
        "    max_mem = torch.cuda.max_memory_allocated(device) / 1e6  # MB\n",
        "    return max_mem\n",
        "\n",
        "max_mem_gc = run_one_batch(True)\n",
        "max_mem_no_gc = run_one_batch(False)\n",
        "print(f\"Max memory with gradient checkpointing: {max_mem_gc:.2f} MB\")\n",
        "print(f\"Max memory without gradient checkpointing: {max_mem_no_gc:.2f} MB\")"
      ],
      "metadata": {
        "id": "3LXXM9DGXbsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Curriculum learning and data scheduling\n",
        "- start from \"easy\" example than go to difficult ones\n",
        "- benefit: quick converge, avoid local optim\n",
        "- how: empirical, data complexity measures, weak model generated loss, external models, etc."
      ],
      "metadata": {
        "id": "Gs5e7SrsfDyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class curriculumDataset(Dataset):\n",
        "    def __init__(self, base_dataset, difficult_order, frac = 0.3):\n",
        "        self.dataset = base_dataset\n",
        "        self.frac = frac\n",
        "        self.difficult_order = difficult_order\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.dataset) * self.frac)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[self.difficult_order[idx]]\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "# train a base model\n",
        "model = SimpleNN(input_size=784, hidden_size=256, output_size=10)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for epoch in range(2): # short for pretraining\n",
        "    for data, target in dataset:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# order loss by computing per-sample loss\n",
        "loss = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for data, target in DataLoader(dataset, batch_size=1, shuffle=False):\n",
        "      output = model(data)\n",
        "      loss.append(criterion(output, target).item())\n",
        "  difficult_order = torch.argsort(torch.tensor(loss))\n",
        "\n",
        "# curriculum learning\n",
        "\n",
        "for frac in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "    c_dataset = curriculumDataset(dataset, difficult_order, frac)\n",
        "    dataloader = DataLoader(c_dataset, batch_size=32, shuffle=True)\n",
        "    for epoch in range(10):\n",
        "      for data, target in dataloader:\n",
        "          optimizer.zero_grad()\n",
        "          output = model(data)\n",
        "          loss = criterion(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n"
      ],
      "metadata": {
        "id": "VVupd6BXfqUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP7MOGUQu9-a"
      },
      "source": [
        "## Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4pMf-ff2nxa"
      },
      "outputs": [],
      "source": [
        "class TextGenerator:\n",
        "    def __init__(self, model, tokenizer, device, max_length=100, eos_token=None):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        if eos_token is None:\n",
        "            self.eos_token = self.tokenizer.eot_token\n",
        "        else:\n",
        "            self.eos_token = eos_token\n",
        "\n",
        "    def generate(self, prompt, num_samples=1, temperature=1.0, top_k=50):\n",
        "        tokens = self.tokenizer.encode(prompt)\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0) # 0, s\n",
        "        for _ in range(num_samples):\n",
        "            for _ in range(self.max_length - len(tokens)):\n",
        "                logits = self.model(tokens)\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                filtered_logits = logits.topk(top_k)[0]\n",
        "                probs = F.softmax(filtered_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                if next_token.item() == self.eos_token:\n",
        "                    break\n",
        "            generated_text = self.tokenizer.decode(tokens[0].tolist())\n",
        "            print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KV cache\n",
        "- During inference, what you only care is the last token, but the prompt KV will be calculated repeatedly for the tokens after next token. That's a waste.\n",
        "- Cache KV's in RAM during the first token generated"
      ],
      "metadata": {
        "id": "Capl5WYmCN7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWithKVCache(MultiHeadAttention):\n",
        "    def __init__(self, d_model, heads, dropout):\n",
        "        super().__init__(d_model, heads, dropout)\n",
        "\n",
        "    def forward_with_kv_cache(self, query, key, value, kv_cache, mask=None):\n",
        "        # b, s, d\n",
        "        batch_size = query.size(0)\n",
        "        # linear\n",
        "        query = self.query_linear(query)\n",
        "        if kv_cache is not None:\n",
        "            assert kv_cache[0].size(1) < key.size(1), \"kv_cache[0] sequence length should be shorter than key\"\n",
        "            assert kv_cache[1].size(1) < value.size(1), \"kv_cache[1] sequence length should be shorter than value\"\n",
        "            new_key = key[:,kv_cache[0].size(1):,:]\n",
        "            new_value = value[:,kv_cache[1].size(1):,:]\n",
        "            new_key = self.key_linear(new_key)\n",
        "            new_value = self.value_linear(new_value)\n",
        "            key = torch.cat((kv_cache[0], new_key), dim=1)\n",
        "            value = torch.cat((kv_cache[1], new_value), dim=1)\n",
        "        else:\n",
        "            key = self.key_linear(key)\n",
        "            value = self.value_linear(value)\n",
        "        # split - allow computing for n heads at the same time.\n",
        "        query = query.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # b,n,s,d\n",
        "        key = key.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # optional\n",
        "        # RoPE for positional encoding\n",
        "        if self.rotary_embedding is not None:\n",
        "          query = self.rotary_embedding(query)\n",
        "          key = self.rotary_embedding(key)\n",
        "        value = value.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)\n",
        "        # attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim) # keep variant stable\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -float('inf'))\n",
        "        else:\n",
        "            # for decoder-style, add tril mask by default in both training and generating\n",
        "            scores = scores.masked_fill(torch.tril(torch.ones_like(scores, device=query.device)) == 0, -1e9)\n",
        "        # softmax\n",
        "        attention = F.softmax(scores, dim=-1) # b, n, s, s\n",
        "        # output - use contiguous to sort RAM to make view faster.\n",
        "        output = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # linear projection\n",
        "        return self.output_linear(output), (key, value)\n",
        "\n",
        "class TransformerBlockWithKVCache(TransformerBlock):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout):\n",
        "        super().__init__(d_model, heads, d_ff, dropout)\n",
        "        self.attention = MultiHeadAttentionWithKVCache(d_model, heads, dropout)\n",
        "\n",
        "    def forward_with_kv_cache(self, x, kv_cache, mask=None):\n",
        "\n",
        "        # Use pre-norm\n",
        "        # input -> norm -> attention -> residue -> norm -> ffn -> residue -> output\n",
        "        # attention\n",
        "        residue = x\n",
        "        x = self.norm1(x)\n",
        "        x, kv_cache = self.attention.forward_with_kv_cache(x, x, x, kv_cache, mask)\n",
        "        x = x + residue\n",
        "        # feed forward\n",
        "        residue = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = x + residue\n",
        "        return x, kv_cache\n",
        "\n",
        "\n",
        "class GPT2WithKVCache(GPT2):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.transformer_blocks_with_kv_cache = nn.ModuleList([TransformerBlockWithKVCache(self.d_model, self.heads, self.d_ff, self.dropout) for _ in range(self.n_layers)])\n",
        "\n",
        "    def forward_with_kv_cache(self, x, kv_cache):\n",
        "        token_emb = self.embedding(x)\n",
        "        if kv_cache is None:\n",
        "          pos_emb = self.position_embedding(torch.arange(x.size(1), device=x.device))\n",
        "        else:\n",
        "          # only need pos emb for new tokens if kv_cache is available.\n",
        "          pos_emb = self.position_embedding(torch.arange(kv_cache[0][0].size(1), kv_cache[0][0].size(1) + x.size(1), device=x.device))\n",
        "        x = token_emb + pos_emb\n",
        "        kv_cache_update = []\n",
        "        for i, transformer in enumerate(self.transformer_blocks_with_kv_cache):\n",
        "            kv_i = kv_cache[i] if kv_cache is not None and i < len(kv_cache) else None\n",
        "            x, kv_i = transformer.forward_with_kv_cache(x, kv_i)\n",
        "            kv_cache_update.append(kv_i)\n",
        "        x = self.laynorm(self.fc(x))\n",
        "        return x, kv_cache_update\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=50, temperature=1.0, top_k=50):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            tokens = self.tokenizer.encode(prompt)\n",
        "            tokens = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0) # 1, s\n",
        "            kv_cache = None\n",
        "            for _ in range(max_new_tokens):\n",
        "              if kv_cache is None:\n",
        "                  # first pass, calculate all tokens\n",
        "                  input_tokens = tokens\n",
        "              else:\n",
        "                  # only new tokens\n",
        "                  input_tokens = tokens[:, -1:] # next token\n",
        "\n",
        "                logits, kv_cache = self.forward_with_kv_cache(input_tokens, kv_cache) # general idea is to store this kv_cache\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                if top_k > 0:\n",
        "                  filtered_logits, top_k_indices = logits.topk(top_k)\n",
        "                  probs = F.softmax(filtered_logits, dim=-1)\n",
        "                  next_token_idx = torch.multinomial(probs, num_samples=1)\n",
        "                  next_token = top_k_indices.gather(-1, next_token_idx)\n",
        "                else:\n",
        "                  probs = F.softmax(logits, dim=-1)\n",
        "                  next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                if next_token.item() == self.eos_token:\n",
        "                    break\n",
        "          generated_text = self.tokenizer.decode(tokens[0].tolist())\n",
        "        return generated_text"
      ],
      "metadata": {
        "id": "xGpKZza5C3-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization\n",
        "- during inference, not full model precision (fp32) is used for forward pass. FP32 were convered to INT4 or INT8 for forward pass and calculation (save memory).\n",
        "- Weight quantization is popular (most memory consumption), activation usually still use FP32\n",
        "- nn.Linear only take fp32, so custermized nn.Linear should be created.\n",
        "- why not activation? it is hard, because the range of activation is dynamic (after weight matmul operation).\n",
        "- dynamic quantization: quantization activation on the fly. FP32 min/max still needed for activation, but heavy duty (matmul) will only use INT4/8 (4 times memory saving) op later.\n",
        "- static quantization: can use dataset to calibrate find min/max of the activation before inference. But might not be stable because the distribution might not be the same.\n",
        "- Pure PyTorch doesn't have optimized integer matmul. integer arithmetic is required to calculate forward pass to really speed up. (Implementation below is a faked one still use fp32)\n",
        "- pytorch has an API to handle model conversion.\n"
      ],
      "metadata": {
        "id": "svdaZGuPebTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class QLinear(nn.Module):\n",
        "    def __init__(self, lin: nn.Linear, precision: str, symmetric: bool):\n",
        "        super(QLinear, self).__init__()\n",
        "        assert isinstance(lin, nn.Linear)\n",
        "        self.in_features  = lin.in_features\n",
        "        self.out_features = lin.out_features\n",
        "        self.precision = precision\n",
        "        self.symmetric = symmetric\n",
        "\n",
        "        # keep bias in fp32\n",
        "        if lin.bias is not None:\n",
        "            self.register_buffer('bias', lin.bias.data.clone())\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        self.lin = lin\n",
        "\n",
        "        with torch.no_grad():\n",
        "            W = lin.weight.data.clone()\n",
        "            self.weight, self.scale, self.zero_point = self.quantization(W, precision, symmetric)\n",
        "        # Store quantization parameters as buffers\n",
        "        self.register_buffer('scale', torch.tensor(self.scale))\n",
        "        self.register_buffer('zero_point', torch.tensor(self.zero_point))\n",
        "\n",
        "        # small tips. buffer vs attr vs parameters\n",
        "        # parameters - trainable tensors, require_grad, move with model, save_dict, in model.parameters\n",
        "        # buffer - not trainable model state, not required_grad, move with model, save_dict, not in model.parameters\n",
        "        # attr - temp staff, not save_dict, not move with model.to(device) - can cause bugs.\n",
        "\n",
        "    def forward(self, x, dq=True):\n",
        "        if dq:\n",
        "            weight = dequantization(self.weight, self.scale, self.zero_point)\n",
        "        else:\n",
        "            weight = self.weight.float()\n",
        "        output = torch.nn.functional.linear(x, weight, self.bias)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def quantization(x, precision, symmetric):\n",
        "        if precision == \"INT8\":\n",
        "            q_max, q_min = -128, 127\n",
        "        elif precision == \"INT4\":\n",
        "            q_max, q_min = -8, 7\n",
        "        else:\n",
        "            raise ValueError(\"Invalid precision\")\n",
        "\n",
        "        min_val = x.min().item()\n",
        "        max_val = x.max().item()\n",
        "\n",
        "        if symmetric: # Forces the quantized range to be symmetric around zero\n",
        "            zero_point = 0\n",
        "            # floating-point 0.0 maps to quantized 0\n",
        "            range = max(abs(min_val), abs(max_val)) # [-range, range]\n",
        "            scale = range / q_max if q_max > 0 else 1.0\n",
        "        else:\n",
        "            scale = (max_val - min_val) / (q_max - q_min) if (max_val - min_val) > 0 else 1.0\n",
        "            zero_point_float = q_min - min_val / scale # (? - q-min) * scale = 0 - min_val\n",
        "            zero_point = izero_point = int(np.clip(np.round(zero_point_float), q_min, q_max))\n",
        "\n",
        "        q = torch.clamp(torch.round(x / scale) + zero_point, q_min, q_max)\n",
        "        q = q.to(torch.int8) # torch does not have int4.\n",
        "        return q, scale, zero_point\n",
        "\n",
        "    @staticmethod\n",
        "    def dequantization(q, scale, zero_point):\n",
        "        return (q.to(torch.float32) - zero_point) * scale\n",
        "\n",
        "def quant_model(module: nn.Module, precision: str, symmetric: bool):\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, nn.Linear):\n",
        "            setattr(module, name, QLinear(child, precision, symmetric))\n",
        "        else:\n",
        "            quant_model(child, precision, symmetric)\n",
        "      return module\n",
        "# an inference example\n",
        "load_model_path = 'xxxx'\n",
        "model_fp32 = SimpleNN(input_size=1024, hidden_size=512, output_size=1024)\n",
        "model_fp32.load_state_dict(torch.load(load_model_path))\n",
        "model_int8 = quant_model(model_fp32, \"INT8\", True)\n",
        "# alternative is to use torch API\n",
        "# Apply dynamic quantization to Linear layers\n",
        "model_int8 = torch.quantization.quantize_dynamic(\n",
        "    model_fp32,  # model in fp32\n",
        "    {nn.Linear}, # which layers to quantize\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "# sample input\n",
        "input_tensor = torch.randn(1, 1024)\n",
        "# forward pass\n",
        "output = model_fp32(input_tensor)\n",
        "# quantized forward pass\n",
        "output_q = model_int8(input_tensor)"
      ],
      "metadata": {
        "id": "s9f6Phk-gMR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Speculative decoding\n",
        "- large model is expensive - each time only one next token generated\n",
        "- use small model to do this next token generation for 5 tokens, and then use large model to verify (calculate logits) for the 5 tokens + prefix so that it was paralleled\n",
        "- accept the same token prediction, and discard the rest.\n",
        "- continue this process\n",
        "- Huggingface has it implemented\n",
        "- small and large model has to share the same tokenizer and vocabulary"
      ],
      "metadata": {
        "id": "-fRw1f28pQ5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "small_model_name = \"gpt2\"\n",
        "large_model_name = \"gpt2-large\"\n",
        "\n",
        "small_tokenizer = AutoTokenizer.from_pretrained(small_model_name)\n",
        "large_tokenizer = AutoTokenizer.from_pretrained(large_model_name)\n",
        "assert small_tokenizer.vocab_size == large_tokenizer.vocab_size, \"Vocab size mismatch\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "small_model = AutoModelForCausalLM.from_pretrained(small_model_name).to(device).eval()\n",
        "large_model = AutoModelForCausalLM.from_pretrained(large_model_name).to(device).eval()\n",
        "\n",
        "prompt = \"Once upon a time\"\n",
        "input_ids = small_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "prompt_tokens_len = input_ids.shape[1]\n",
        "max_total_new_tokens = 50\n",
        "drafted_tokens = 5\n",
        "\n",
        "with torch.inference_mode():\n",
        "  while input_ids.shape[1] < prompt_tokens_len + max_total_new_tokens:\n",
        "      with torch.no_grad():\n",
        "          max_new_tokens_remained = max_total_new_tokens - (input_ids.shape[1] - prompt_tokens_len)\n",
        "          max_new_tokens = min(drafted_tokens, max_new_tokens_remained)\n",
        "          small_output_ids = small_model.generate(input_ids, max_new_tokens=max_new_tokens, do_sample=True, top_k=50, use_cache=True, pad_token_id=small_tokenizer.eos_token_id)\n",
        "\n",
        "          # combine with prompt\n",
        "          candidate_input_ids = small_output_ids\n",
        "          drafted_output_ids = small_output_ids[:, input_ids.shape[1]:]\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # forward pass\n",
        "          large_output = large_model(candidate_input_ids, use_cache=True)\n",
        "          # get logits\n",
        "          large_new_tokens_logits = large_output.logits[:, (input_ids.shape[1] - 1):-1, :]\n",
        "          large_output_ids = large_new_tokens_logits.argmax(dim=-1)\n",
        "          # safeguard for EOS\n",
        "          if drafted_output_ids.numel() == 0:\n",
        "              added_tokens = large_output.logits[:,-1,:].argmax(dim=-1, keepdim=True)\n",
        "              input_ids = torch.cat([input_ids, added_tokens], dim=1)\n",
        "              continue\n",
        "\n",
        "          # verify tokens\n",
        "          equal_mask = drafted_output_ids[0] == large_output_ids[0]\n",
        "          # find the first mismatch in equal_mask if empty return None\n",
        "          mismatch_idx = torch.where(equal_mask == False)[0].item() if torch.any(equal_mask == False) else None\n",
        "          accepted_small_output_ids = drafted_output_ids[:, :mismatch_idx] if mismatch_idx is not None else drafted_output_ids\n",
        "          extra_one_large_output_ids = large_output_ids[:, mismatch_idx:mismatch_idx+1] if mismatch_idx is not None else large_output.logits[:,-1,:].argmax(dim=-1, keepdim=True)\n",
        "          if accepted_small_output_ids.shape[1] == 0:\n",
        "              added_tokens = extra_one_large_output_ids\n",
        "          else:\n",
        "              added_tokens = torch.cat([accepted_small_output_ids, extra_one_large_output_ids], dim=1)\n",
        "\n",
        "          # update input_ids\n",
        "          input_ids = torch.cat([input_ids, added_tokens], dim=1)\n",
        "  # decode input_ids\n",
        "  generated_text = small_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "  print(generated_text)\n",
        "\n",
        "\n",
        "  # a easy way is to use huggingface API - key argument assistant_model\n",
        "  output_ids = large_model.generate(input_ids, max_new_tokens=max_total_new_tokens, do_sample=True, top_k=50, use_cache=True, pad_token_id=large_tokenizer.eos_token_id, assistant_model=small_model)\n",
        "  generated_text = large_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "  print(generated_text)"
      ],
      "metadata": {
        "id": "tThTO6z4psPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONXX\n",
        "- an exchangable format can be used in many runtimes (javascript, java, etc.)\n",
        "- export onxx and a dummy input. dummy input is forward once to verify the model, and define the shape as well as allowing pytorch to see layers.\n",
        "- use session to load it\n",
        "- use dynamic_axes to allow flexibility. otherwise in runtime, it only allow batch_size = 1\n",
        "- opset_version define the set of ops (nn.Linear, ) implemented in onxx. Some might not be available can lead to failure in export and inference - not all models are supported by onxx ops.\n",
        "- use newer opset to cover more implemented layers. but runtime must also support it. (exchangable is not actually exchangable, e.g. too fancy javascript might not be supported by old browswer)"
      ],
      "metadata": {
        "id": "e-Lsv54fJqlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "input = torch.randn(1, 1024) # a dummy\n",
        "model = SimpleNN(input_size=1024, hidden_size=512, output_size=1024)\n",
        "model.eval()\n",
        "\n",
        "torch.onnx.export(model, torch.randn(1, 1024), \"simple_nn.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"],\n",
        "                  opset_version = 17, dynamic_axes={\"input\" : {0 : \"batch_size\"}, \"output\" : {0 : \"batch_size\"}})\n",
        "\n",
        "# during inference load onnx\n",
        "import onnxruntime\n",
        "session = onnxruntime.InferenceSession(\"simple_nn.onnx\")\n",
        "output = session.run([\"output\"], {\"input\": input.numpy()}) # validate it\n",
        "new_input = torch.randn(10,1024)\n",
        "output = session.run([\"output\"], {\"input\": new_input.numpy()})\n",
        "# use None to get all output\n",
        "output = session.run(None, {\"input\": new_input.numpy()}"
      ],
      "metadata": {
        "id": "F7WOYDMXJ_tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorRT\n",
        "- One of the runtimes for build/compile onxx and run it in NVIDIA GPUs (onnxruntime is more general, really about generalizability)\n",
        "- There are other runtimes for different hardware (e.g. CPU, android)\n",
        "- TensorRT is like C++, while pytorch+cuda is like python. onxx is like the source code, TensorRT can build an engine, like executable (.cpp or .exe) files compiled, which is runtime optimized\n",
        "- Benefits of runtime: precompiled, kernel fused, memory planning, precision lowering, etc. But difficult to debug, so often used in deployment"
      ],
      "metadata": {
        "id": "Ek2nCm36QeH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%trtexec --onxx=simple_nn.onnx --saveEngine=simple_nn.engine --minShapes=input:1x1024 --optShapes=input:1x1024 --maxShapes=input:1x1024 --fp16"
      ],
      "metadata": {
        "id": "0Sq-Po3ZRlKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "\n",
        "engine_bytes = open(\"simple_nn.engine\", \"rb\").read()\n",
        "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n",
        "engine = runtime.deserialize_cuda_engine(engine_bytes)\n",
        "context = engine.create_execution_context()\n",
        "\n",
        "input_shape = (1, 1024)\n",
        "input_data = np.random.randn(*input_shape).astype(np.float32)\n",
        "d_input = cuda.mem_alloc(input_data.nbytes)\n",
        "d_output = cuda.mem_alloc(input_data.nbytes)\n",
        "bindings = [int(d_input), int(d_output)]\n",
        "context.set_binding_shape(0, input_shape)\n",
        "cuda.memcpy_htod(d_input, input_data)\n",
        "context.execute_v2(bindings)\n",
        "cuda.memcpy_dtoh(out, d_out)\n"
      ],
      "metadata": {
        "id": "jj4ixxCIS4I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oU6EZVWvDJq"
      },
      "source": [
        "## Training Shakespear GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOD1VLrkuN6p"
      },
      "outputs": [],
      "source": [
        "# define all training parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# get gpt2 tokenizer and vocab_size\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab\n",
        "# model parameters\n",
        "model_config = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"d_model\": 128,\n",
        "    \"max_seq\": 128,\n",
        "    \"n_layers\": 4,\n",
        "    \"heads\": 4,\n",
        "    \"d_ff\": 256,\n",
        "    \"dropout\": 0.1,\n",
        "    \"rope\": True\n",
        "}\n",
        "# training parameters\n",
        "batch_size=2\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 1000\n",
        "max_steps = 10000\n",
        "gradient_accumulation_steps = 5\n",
        "grad_clip = 1.0\n",
        "epochs = 2\n",
        "# train and val data\n",
        "if os.path.exists(\"data/t8.shakespeare.txt\"):\n",
        "    # if the file exists, read from it\n",
        "    with open(\"data/t8.shakespeare.txt\", \"r\") as f:\n",
        "        data = f.read()\n",
        "else:\n",
        "    shakespear_content_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "    # read the content from the URL\n",
        "    data = requests.get(shakespear_content_url).text\n",
        "    # save to content to local file\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "    with open(\"data/t8.shakespeare.txt\", \"w\") as f:\n",
        "        f.write(data)\n",
        "train_data, val_data = data[:int(len(data)*0.8)], data[int(len(data)*0.8):]\n",
        "train_dataloader = DataLoader(TextDataset(train_data[1:10000], tokenizer, model_config['max_seq']), batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(TextDataset(val_data[1:1000], tokenizer, model_config['max_seq']), batch_size=batch_size, shuffle=True)\n",
        "model = GPT2(**model_config)\n",
        "# init model and trainer\n",
        "trainer = Trainer(model, device, learning_rate, weight_decay, warmup_steps, max_steps, gradient_accumulation_steps, grad_clip, \"checkpoints\")\n",
        "# load model if available\n",
        "if os.path.exists(\"checkpoints/best_model.pt\"):\n",
        "    try:\n",
        "      trainer.load_checkpoint()\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "# start training\n",
        "trainer.train(train_dataloader, val_dataloader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUm-ifzevLy2"
      },
      "source": [
        "## Inference on Trained GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWbDPn2JwHu6"
      },
      "outputs": [],
      "source": [
        "# generate\n",
        "# load bst model\n",
        "model = GPT2(**model_config)\n",
        "model.load_state_dict(torch.load(\"checkpoints/best_model.pt\")['model_state_dict'])\n",
        "eos_token = tokenizer.eot_token\n",
        "text_generator = TextGenerator(model, tokenizer, device, max_length=100, eos_token=eos_token)\n",
        "prompt = \"he is a \"\n",
        "text_generator.generate(prompt, num_samples=1, temperature=1.0, top_k=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RLHF\n",
        "- beyond unsupervised traning - next tokens only\n",
        "- SFT: supervised fine-tuning. training dataset includes prompt + response. loss is calculated based on response part. This is called policy.\n",
        "- After SFT, model is able to generate candidates for a given prompt\n",
        "- reward scores will be assigned to the response (human assigned, or based on some other mechanism). say candidate A is better then candidate B, etc.\n",
        "- train a reward model (e.g. project the last hidden layer in GPT to a logit) to maximize loss in negative candidates, and minimize loss in positive candidates\n",
        "- Note, both SFT, and Reward model takes prompt + generation as the whole sequence b/c overall it is still next token generation GPT style model.\n",
        "- We can then update policy model by PPO (proximal policy optimization).\n",
        "  - step 0: ref_policy and current_policy are both SFT policy\n",
        "  - step 1: rollout all candidates\n",
        "  - step 2: calculate reward score\n",
        "  - step 3: penalt this reward with a KL divergence between current and reference policy (avoid the model being trivial close to reward model);\n",
        "  - step 4: generate new candidates and update the current policy to align with a good reward and go away from a bad reward. (but add the current policy as the constraint to make sure it won't go too far away); add clip to stablize it\n",
        "  - step 6: backpropate to update current policy\n",
        "  - step 7: repeat 4-7\n",
        "  - step 8: repeat 1-7"
      ],
      "metadata": {
        "id": "megAqa4TCZFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### an tiny example\n",
        "\"\"\"\n",
        "RLHF (toy) using Hugging Face GPT-2\n",
        "-----------------------------------\n",
        "This is an end-to-end *minimal* RLHF-style pipeline using `transformers` GPT-2:\n",
        "  1) Supervised fine-tuning (SFT) of GPT-2 on prompt→response pairs\n",
        "  2) Train a reward model (RM) with pairwise preferences (synthetic in this demo)\n",
        "  3) PPO-style RL fine-tuning of the policy against the RM with a KL penalty to a frozen reference policy\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import math, random, copy\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "\n",
        "SFT_DATA = [\n",
        "    (\"Say something nice about dogs.\", \"Dogs are wonderful companions and loyal friends.\"),\n",
        "    (\"Give a polite greeting.\", \"Hello! I hope you're having a great day.\"),\n",
        "    (\"Encourage someone studying.\", \"Keep going—you are learning more every day!\"),\n",
        "    (\"Talk kindly about teamwork.\", \"Working together helps everyone grow and succeed.\"),\n",
        "    (\"Thank a teacher.\", \"Thank you for sharing knowledge with patience and care.\"),\n",
        "    (\"Compliment a friend.\", \"You bring warmth and kindness wherever you go.\"),\n",
        "]\n",
        "PROMPTS = [p for (p, _) in SFT_DATA]\n",
        "\n",
        "POS_WORDS = set(\"great wonderful loyal kind kindness warm warmth nice helpful thanks thank you patient patience care caring grow succeed love amazing awesome brilliant delightful excellent happy hope encourage together\".split())\n",
        "NEG_WORDS = set(\"bad horrible rude mean cruel lazy stupid dumb terrible hate awful annoying useless\".split())\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"gpt2\"  # small GPT-2 (124M). For even smaller, consider distilgpt2.\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "policy = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "policy.resize_token_embeddings(len(tok)) # ebsure the model embedding aligned with tokenizer\n",
        "policy.train()\n",
        "\n",
        "# Reference (frozen) will be set after SFT\n",
        "ref_policy = None\n",
        "\n",
        "class GPT2RewardModel(nn.Module):\n",
        "  def __init__(self,policy):\n",
        "    super().init()\n",
        "    hidden_size = policy.last_hidden_state.shape[0]\n",
        "    self.proj = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(input_ids):\n",
        "    output = policy(input_ids)\n",
        "    prob = self.proj(output.last_hidden_state)\n",
        "    return prob\n",
        "\n",
        "class SftDataLoader(DataLoader):\n",
        "  def __init__(self,SFT_DATA):\n",
        "    self.data = SFT_DATA\n",
        "    self.prompt_length = [len(ele[0]) for ele in self.data]\n",
        "\n",
        "  def __len__():\n",
        "    return len(data)\n",
        "\n",
        "  def __get__(idx):\n",
        "    input = self.data[idx, :-1]\n",
        "    label = self.data[idx, 1:]\n",
        "    return input, label, prompt_length[idx]\n",
        "\n",
        "class PrefDataLoader(DataLoader):\n",
        "  def __init__(self, SFT_DATA):\n",
        "    self.data = SFT_DATA\n",
        "    self.generation = [d[1] for d in self.data]\n",
        "    self.score = [_syn_score(g) for g in generation]\n",
        "  def _sy_score(generation):\n",
        "    for t in generation.split():\n",
        "      if t in POS_WORDS:\n",
        "        score +=1\n",
        "      elif t in NEG_WORDS:\n",
        "        score -=1\n",
        "      else:\n",
        "        score +=0.1\n",
        "    return score\n",
        "\n",
        "  def __len__():\n",
        "    return len(data)\n",
        "\n",
        "  def __get__(idx):\n",
        "    input = self.data[idx]\n",
        "    score = self.score[idx]\n",
        "    return input, score\n",
        "\n",
        "class RolloutDataLoader(DataLoader):\n",
        "  def __init__(self,SFT_DATA):\n",
        "    self.prompt = [d[0] for d in self.data]\n",
        "\n",
        "\n",
        "# Train SFT\n",
        "policy.train()\n",
        "optim = self.AdamW(parameters = policy.parameters, lr=0.001)\n",
        "for idx, batch in enumerate(SftDataLoader(SFT_DATA)):\n",
        "  input_ids, labels, prompt_length = batch[0], batch[1], batch[2]\n",
        "  output = policy(input_ids)\n",
        "  # calculate loss for generation only\n",
        "  loss = F.cross_entropy(output[:, prompt_length - 1], label[: prompt_length - 1])\n",
        "  loss.back(); optim.zero_grad(); optim.step()\n",
        "\n",
        "# Train Reward Model\n",
        "rm = GPT2RewardModel(policy)\n",
        "optim = self.AdamW(parameters = rm.parameters, lr=0.001)\n",
        "for idx, batch in enumerate(PrefDataLoader(SFT_DATA)):\n",
        "  input_ids, scores = batch[0], batch[1]\n",
        "  output = rm(input_ids)\n",
        "  loss = F.mse_loss(output, scores)\n",
        "  loss.back(); optim.zero_grad(); optim.step()\n",
        "\n",
        "# PPO against RM\n",
        "\n",
        "ref_policy = policy\n",
        "for idx, batch in enumerate(SftDataLoader(SFT_DATA)):\n",
        "  policy.eval()\n",
        "  input_ids, labels, prompt_length = batch[0], batch[1], batch[2]\n",
        "  # roll out\n",
        "  generation = policy(input_ids[:prompt_length-1])\n",
        "  generation_ref = ref_policy(input_ids[:prompt_length-1])\n",
        "  # calculate reward\n",
        "  logp = output[:, prompt_length - 1].dim(0)\n",
        "  logp_ref = output_ref[:, prompt_length - 1].dim(0)\n",
        "  reward = rm(input_ids[:prompt_length-1] + generation) + 0.01* (logp - logp_ref)\n",
        "  policy.train()\n",
        "  optim = self.AdamW(parameters = policy.parameters, lr=0.001)\n",
        "  for idx, batch in enumerate(SftDataLoader(SFT_DATA)):\n",
        "    new_input_ids, new_labels, new_prompt_length = batch[0], batch[1], batch[2]\n",
        "    output = policy(new_input_ids)\n",
        "    logp_new = policy(new_input_ids[:, prompt_length - 1])\n",
        "    ratio = tensor.clip((logp_new - logp), -0.2, 0.2)\n",
        "    loss =  ratio * reward\n",
        "    loss.back(); optim.zero_grad(); optim.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eg6BmBP8Ccw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUu0O7k0zM7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "12ezKk_kmj9oUbRsIM6-sAmzGxxSjZ0QR",
      "authorship_tag": "ABX9TyPRrFs26GChJuQC4NfKMNWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}